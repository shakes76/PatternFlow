{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages and libaray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "import importlib\n",
    "import gan_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated images for a display interval every 1000 epochs\n",
    "\n",
    "def display_images(n): \n",
    "        r, c = 1,4\n",
    "        noise = np.random.normal(0, 1, (r * c,100)) \n",
    "        generated_images = generator.predict(noise) \n",
    "      \n",
    "        imgs = generator.predict(noise)\n",
    "        fig = plt.figure(figsize=(40,10))\n",
    "        for i, img in enumerate(generated_images):\n",
    "            ax = fig.add_subplot(1,4,i+1)\n",
    "            ax.imshow(img,cmap=\"gray\")\n",
    "        fig.suptitle(\"Generated images: %s\"%n,fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading images.\n",
    "\n",
    "def data_gen(img_folder, batch_size):\n",
    "    c = 0\n",
    "    n = os.listdir(img_folder) #List of training images\n",
    "    np.random.shuffle(n)\n",
    "    print(\"Number of Train_images:\",len(n))\n",
    "    \n",
    "    while (True):\n",
    "        img = np.zeros((batch_size, 256, 256, 1)).astype('float')\n",
    "       \n",
    "         #Reading set of images for a mentioned batch-size.\n",
    "        for i in range(c, c+batch_size):\n",
    "            train_img = tf.keras.preprocessing.image.img_to_array(tf.keras.preprocessing.image.load_img(\n",
    "                img_folder+'/'+n[i],color_mode='grayscale', target_size=(256, 256)))/255\n",
    "            \n",
    "            #add to array - img[0], img[1], and so on.\n",
    "            img[i-c] = train_img \n",
    "             \n",
    "        c+=batch_size\n",
    "        if(c+batch_size>=len(os.listdir(img_folder))):\n",
    "            c=0\n",
    "            np.random.shuffle(n)\n",
    "                  \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                         optimizer=tf.keras.optimizers.Adam(0.0002,0.5))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real image samples\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # select images\n",
    "    X = dataset[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "\n",
    "def generate_noise_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.normal(0, 1, (n_samples, 100))\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake image samples\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_noise_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "    # plot Discrimination loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(d1_hist, label='d-real_loss')\n",
    "    plt.plot(d2_hist, label='d-fake_loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plot Generator loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(g_hist, label='gen_loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #plt.subplot(3, 1, 3)\n",
    "    plt.plot(d1_hist, label='d-real_loss')\n",
    "    plt.plot(d2_hist, label='d-fake_loss')\n",
    "    plt.plot(g_hist, label='gen_loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining path for accessing dataset\n",
    "image_dir = 'H:/Downloads/keras_png_slices_data/Dataset/keras_png_slices_train/image'\n",
    "\n",
    "#Parameters\n",
    "num_epochs=15000\n",
    "batch_size=16\n",
    "display_interval=1000\n",
    "train_gen = data_gen(image_dir, batch_size = 16)\n",
    "\n",
    "#Building Model\n",
    "generator = gan_model.make_generator_model()\n",
    "discriminator = gan_model.make_discriminator_model()\n",
    "combined_network = define_gan(generator, discriminator)\n",
    "\n",
    "# prepare lists for storing stats each iteration\n",
    "d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "\n",
    "              \n",
    "# calculate the number of batches per epoch\n",
    "bat_per_epo = int(train_gen.shape[0] / batch_size)\n",
    "\n",
    "# calculate the total iterations based on batch and epoch\n",
    "n_steps = bat_per_epo * num_epochs\n",
    "\n",
    "for i in range(n_steps+1):             \n",
    "\n",
    "            #Sampling a random half of images and Training the discriminator\n",
    "            X_real, y_real = generate_real_samples(train_gen, int(batch_size / 2))\n",
    "            d_loss1, d_acc1 = discriminator.train_on_batch(X_real, y_real)\n",
    "            \n",
    "            #Sampling noise, generating a batch of new fake images and Training the discriminator\n",
    "            X_fake, y_fake = generate_fake_samples(generator, 100, int(batch_size / 2))\n",
    "            d_loss2, d_acc2 = discriminator.train_on_batch(X_fake, y_fake)\n",
    "               \n",
    "            #Training the generator to generate images which pass the authenticity test \n",
    "            X_gan = generate_noise_points(100, batch_size)\n",
    "            y_gan = ones((batch_size, 1))\n",
    "            g_loss = combined_network.train_on_batch(X_gan, y_gan) \n",
    "            \n",
    "            #Tracking the progress                 \n",
    "            if i % display_interval == 0:\n",
    "                #print(\"Value of i:\", i,\"Value of Display_Interval: \", display_interval)\n",
    "                display_images(i) \n",
    "                    \n",
    "            # record history\n",
    "            d1_hist.append(d_loss1)\n",
    "            d2_hist.append(d_loss2)\n",
    "            g_hist.append(g_loss)\n",
    "            a1_hist.append(d_acc1)\n",
    "            a2_hist.append(d_acc2)\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Loss\n",
    "plot_loss(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
