from __future__ import division
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "2"

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from model import IUNet


# ===== normalize over the dataset
def dataset_normalized(imgs):
    imgs_normalized = np.empty(imgs.shape)
    imgs_std = np.std(imgs)
    imgs_mean = np.mean(imgs)
    imgs_normalized = (imgs - imgs_mean) / imgs_std
    for i in range(imgs.shape[0]):
        imgs_normalized[i] = ((imgs_normalized[i] - np.min(imgs_normalized[i])) / (
                np.max(imgs_normalized[i]) - np.min(imgs_normalized[i]))) * 255
    return imgs_normalized


####################################  Load Data #####################################
te_data = np.load('data_test.npy')
te_mask = np.load('mask_test.npy')

print('ISIC18 Dataset loaded')

te_data2 = dataset_normalized(te_data)

# Build and compile model
model = IUNet(256, 192)
model.my_compile()

model.load_weights('weight_isic18')
predictions = model.predict(te_data, batch_size=8)

y_scores = predictions.reshape(
    predictions.shape[0] * predictions.shape[1] * predictions.shape[2] * predictions.shape[3], 1)
print(y_scores.shape)

y_true = te_mask.reshape(te_mask.shape[0] * te_mask.shape[1] * te_mask.shape[2] * te_mask.shape[3], 1)

y_scores = np.where(y_scores > 0.5, 1, 0)
y_true = np.where(y_true > 0.5, 1, 0)

output_folder = 'output/'

# Area under the ROC curve
fpr, tpr, thresholds = roc_curve((y_true), y_scores)
AUC_ROC = roc_auc_score(y_true, y_scores)
print("\nArea under the ROC curve: " + str(AUC_ROC))
roc_curve = plt.figure()
plt.plot(fpr, tpr, '-', label='Area Under the Curve (AUC = %0.4f)' % AUC_ROC)
plt.title('ROC curve')
plt.xlabel("FPR (False Positive Rate)")
plt.ylabel("TPR (True Positive Rate)")
plt.legend(loc="lower right")
plt.savefig(output_folder + "ROC.png")

# Precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
precision = np.fliplr([precision])[0]
recall = np.fliplr([recall])[0]
AUC_prec_rec = np.trapz(precision, recall)
print("\nArea under Precision-Recall curve: " + str(AUC_prec_rec))
prec_rec_curve = plt.figure()
plt.plot(recall, precision, '-', label='Area Under the Curve (AUC = %0.4f)' % AUC_prec_rec)
plt.title('Precision - Recall curve')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc="lower right")
plt.savefig(output_folder + "Precision_recall.png")

# Confusion matrix
threshold_confusion = 0.5
print("\nConfusion matrix:  Custom threshold (for positive) of " + str(threshold_confusion))
y_pred = np.empty((y_scores.shape[0]))
for i in range(y_scores.shape[0]):
    if y_scores[i] >= threshold_confusion:
        y_pred[i] = 1
    else:
        y_pred[i] = 0
confusion = confusion_matrix(y_true, y_pred)
print(confusion)
accuracy = 0
if float(np.sum(confusion)) != 0:
    accuracy = float(confusion[0, 0] + confusion[1, 1]) / float(np.sum(confusion))
print("Global Accuracy: " + str(accuracy))
specificity = 0
if float(confusion[0, 0] + confusion[0, 1]) != 0:
    specificity = float(confusion[0, 0]) / float(confusion[0, 0] + confusion[0, 1])
print("Specificity: " + str(specificity))
sensitivity = 0
if float(confusion[1, 1] + confusion[1, 0]) != 0:
    sensitivity = float(confusion[1, 1]) / float(confusion[1, 1] + confusion[1, 0])
print("Sensitivity: " + str(sensitivity))
precision = 0
if float(confusion[1, 1] + confusion[0, 1]) != 0:
    precision = float(confusion[1, 1]) / float(confusion[1, 1] + confusion[0, 1])
print("Precision: " + str(precision))

# Jaccard similarity index
jaccard_index = jaccard_score(y_true, y_pred, pos_label=1)
print("\nJaccard similarity score: " + str(jaccard_index))

# F1 score
F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)
print("\nF1 score (F-measure): " + str(F1_score))

# Save the results
file_perf = open(output_folder + 'performances.txt', 'w')
file_perf.write("Area under the ROC curve: " + str(AUC_ROC)
                + "\nArea under Precision-Recall curve: " + str(AUC_prec_rec)
                + "\nJaccard similarity score: " + str(jaccard_index)
                + "\nF1 score (F-measure): " + str(F1_score)
                + "\n\nConfusion matrix:"
                + str(confusion)
                + "\nACCURACY: " + str(accuracy)
                + "\nSENSITIVITY: " + str(sensitivity)
                + "\nSPECIFICITY: " + str(specificity)
                + "\nPRECISION: " + str(precision)
                )
file_perf.close()

# Save 10 results with error rate lower than threshold
threshold = 300
predictions = np.where(predictions > 0.5, 1, 0)
te_mask = np.where(te_mask > 0.5, 1, 0)
good_prediction = np.zeros([predictions.shape[0], 1], np.uint8)
id_m = 0
for idx in range(predictions.shape[0]):
    esti_sample = predictions[idx]
    true_sample = te_mask[idx]
    esti_sample = esti_sample.reshape(esti_sample.shape[0] * esti_sample.shape[1] * esti_sample.shape[2], 1)
    true_sample = true_sample.reshape(true_sample.shape[0] * true_sample.shape[1] * true_sample.shape[2], 1)
    er = 0
    for idy in range(true_sample.shape[0]):
        if esti_sample[idy] != true_sample[idy]:
            er = er + 1
    if er < threshold:
        good_prediction[id_m] = idx
        id_m += 1

fig, ax = plt.subplots(5, 3, figsize=[50, 50])

for idx in range(5):
    ax[idx, 0].imshow(np.uint8(te_data[good_prediction[idx, 0]]))
    ax[idx, 1].imshow(np.squeeze(te_mask[good_prediction[idx, 0]][:, :, 0]), cmap='gray')
    ax[idx, 2].imshow(np.squeeze(predictions[good_prediction[idx, 0]][:, :, 0]), cmap='gray')

plt.savefig(output_folder + 'sample_results.png')

print("end test")
