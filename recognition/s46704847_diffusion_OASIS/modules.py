"""diffusion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ebIE7dlwSnsaJe7G0ngCuGTSa9qdSVKz

Reference: https://medium.com/@vedantjumle/image-generation-with-diffusion-
            models-using-keras-and-tensorflow-9f60aae72ac
"""

__author__ = "Zhao Wang, 46704847"
__email__ = "s4670484@student.uq.edu.au"

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Layer, LayerNormalization
import tensorflow.keras.layers as nn
from tensorflow import keras, einsum
import tensorflow_addons as tfa
from einops import rearrange
import math
from functools import partial
from inspect import isfunction

"""Generate beta, alpha and forward noise"""

timesteps = 300

# create beta 
beta = np.linspace(0.0001, 0.02, timesteps)

# calculate alpha
alpha = 1 - beta
alpha_bar = np.cumprod(alpha, 0)
alpha_bar = np.concatenate((np.array([1.]), alpha_bar[:-1]), axis=0)
sqrt_alpha_bar = np.sqrt(alpha_bar)
one_minus_sqrt_alpha_bar = np.sqrt(1-alpha_bar)

def set_seed(seed):
    """
    Setting random state seed
    Parameters:
        seed (int): random seed
    Returns:
        None
    """
    np.random.seed(seed)

def forward_noise(seed, x_0, t):
    """
    Adding noise to the imput image to timestamp t.
    Parameters:
        seed (int): the ramdom seed
        x_0 (np.array): the input image
        t (int): the timestamp
    Returns:
        noisy_image (np.array): the image added noise
        noise (np.array): the noise
    """
    set_seed(seed)
    noise = np.random.normal(size=x_0.shape)
    reshaped_sqrt_alpha_bar_t = np.reshape(
                                np.take(sqrt_alpha_bar, t), (-1, 1, 1, 1))
    reshaped_one_minus_sqrt_alpha_bar_t = np.reshape(np.take(
                                one_minus_sqrt_alpha_bar, t), (-1, 1, 1, 1))
    noisy_image = reshaped_sqrt_alpha_bar_t  * x_0 + \
                                reshaped_one_minus_sqrt_alpha_bar_t  * noise
    return noisy_image, noise

def generate_timestamp(seed, num):
    """
    Generating a random timestamp series.
    Parameters:
        seed (int): the random seed
        num (int): the number of timestamps
    Returns
        (tr.Tensor): a list of random timestamps
    """
    set_seed(seed)
    return tf.random.uniform(shape=[num], minval=0, maxval=timesteps, 
                                                            dtype=tf.int32)

# Visualize the forward noise process
def show_forward_noise(train_images):
    """
    Plotting the forward progress of adding noise into a image.
    Parameters:
        train_images (np.array): the image dataset
    Returns:
        None
    """
    step = timesteps // 10
    plan = [i*step for i in range(9)]
    fig, axes = plt.subplots(3, 3, figsize=(9,9))
    axes[0,0].imshow(train_images[0,:,:,0], cmap="gray")
    axes[0,0].set_title("Original")
    axes[0,0].axis('off')
    for i in range(3):
        for j in range (3):
            if i + j > 0:
                time_stamp = plan[i*3+j]
                noise_img, noise = forward_noise(0, train_images[0], time_stamp)
                axes[i,j].imshow(noise_img[0,:,:,0], cmap="gray")
                axes[i,j].set_title(f"Timestamp {time_stamp}")
                axes[i,j].axis('off')
    plt.show()


"""Constructing U-Net model"""
#Helper functions

def exists(x):
    return x is not None

def default(val, d):
    if exists(val):
        return val
    return d() if isfunction(d) else d

# We will use this to convert timestamps to time encodings
class SinusoidalPosEmb(Layer):
    def __init__(self, dim, max_positions=10000):
        super(SinusoidalPosEmb, self).__init__()
        self.dim = dim
        self.max_positions = max_positions

    def call(self, x, training=True):
        x = tf.cast(x, tf.float32)
        half_dim = self.dim // 2
        emb = math.log(self.max_positions) / (half_dim - 1)
        emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)
        emb = x[:, None] * emb[None, :]

        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)

        return emb
        
# small helper modules
class Identity(Layer):
    def __init__(self):
        super(Identity, self).__init__()

    def call(self, x, training=True):
        return tf.identity(x)


class Residual(Layer):
    def __init__(self, fn):
        super(Residual, self).__init__()
        self.fn = fn

    def call(self, x, training=True):
        return self.fn(x, training=training) + x

def Upsample(dim):
    return nn.Conv2DTranspose(filters=dim, kernel_size=4, 
                                                  strides=2, padding='SAME')

def Downsample(dim):
    return nn.Conv2D(filters=dim, kernel_size=4, strides=2, padding='SAME')

class PreNorm(Layer):
    def __init__(self, dim, fn):
        super(PreNorm, self).__init__()
        self.fn = fn
        self.norm = LayerNormalization()

    def call(self, x, training=True):
        x = self.norm(x)
        return self.fn(x)

class SiLU(Layer):
    def __init__(self):
        super(SiLU, self).__init__()

    def call(self, x, training=True):
        return x * tf.nn.sigmoid(x)

def gelu(x, approximate=False):
    if approximate:
        coeff = tf.cast(0.044715, x.dtype)
        return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * 
                                                              tf.pow(x, 3))))
    else:
        return 0.5 * x * (1.0 + tf.math.erf(x / 
                                        tf.cast(1.4142135623730951, x.dtype)))

class GELU(Layer):
    def __init__(self, approximate=False):
        super(GELU, self).__init__()
        self.approximate = approximate

    def call(self, x, training=True):
        return gelu(x, self.approximate)