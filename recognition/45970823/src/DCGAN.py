"""
    Author: Dream Gonsalves
    Student id : 45970823
    This is the DCGAN model functions definitions
    It includes: Loading data function, Generator model, Discriminator model, loss functions,ssim calculation function,plot functions
    Reference: https://www.tensorflow.org/tutorials/generative/dcgan

"""
import glob
import cv2
import tensorflow as tf
import imageio
import matplotlib.pyplot as plt
import numpy as np
import PIL
from tensorflow.keras import layers
import time
from IPython import display


def load_image(data_dir,batch_size,buffer_size):
    """
        The Train data is huge in size 
        This function helps in getting the data from the directory and pre-process it

    """
    Train = [cv2.imread(file) for file in glob.glob(data_dir + "/*.png")]

    # converting RGB to grayscale
    X_train = [cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in Train]

    # resizing images to 28 x 28
    X_train = [cv2.resize(image, (128, 128)) for image in X_train]

    # converting to tensors
    tf_X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)
    #print(tf_X_train.shape)

    tf_X_train = tf.reshape(tf_X_train, [tf_X_train.shape[0], 128, 128, 1])
    #print('max = ', tf.reduce_max(tf_X_train).numpy())
    # print('min = ', tf.reduce_min(tf_X_train).numpy())

    # Normalising the dataset
    tf_X_train = (tf_X_train - 127.5)/127.5
    # print('max = ', tf.reduce_max(tf_X_train).numpy())
    # print('min = ', tf.reduce_min(tf_X_train).numpy())

    # Batch and shuffle the data
    train_dataset = tf.data.Dataset.from_tensor_slices(tf_X_train).shuffle(buffer_size).batch(batch_size)
    return tf_X_train,train_dataset

def generator_model():
    """
        It is a squential model built using tensorflow and keras
        It generates fake images 

    """
    model = tf.keras.Sequential()
    model.add(layers.Dense(16 * 16 * 256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((16, 16, 256)))
    assert model.output_shape == (None, 16, 16, 256)  # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 16, 16, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 32, 32, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(4, 4), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 128, 128, 1)

    return model


def generator_loss(fake_output):
    """
        This function calculates the generator loss
    """

    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_model():
    """
        This function uses convolution neural network to classify the generated fake images and the train images.        
    """
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                            input_shape=[128, 128, 1]))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

def discriminator_loss(real_output, fake_output):
    """
      This function calculates the discriminator loss.
      
    """
    # This method returns a helper function to compute cross entropy loss


    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss


def get_ssim_max(input, test):
    """
       This function gives the maximum ssim value generated by comparing the train with the test image.
    """
    ssim_val = 0

    for i in range(len(input)):
        ssim = tf.image.ssim(input[i], test, 1).numpy()
        if (ssim > ssim_val):
            ssim_val = ssim

    return ssim_val



def loss_plots(gen_losses,disc_losses):
    """
       This function generates the plots for generator and discriminator loss
    """
    plt.plot(gen_losses)
    plt.plot(disc_losses)
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['Generator', 'Discriminator'], loc='upper right')
    plt.show()

def ssim_plot(ssims_max):
    """
       This function generates the plots for ssim
    """
    plt.plot(ssims_max)
    plt.title('SSIM')
    plt.ylabel('ssim')
    plt.xlabel('epoch')
    plt.show()











