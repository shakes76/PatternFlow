2021-10-22 15:00:22.448644: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
Loading image paths...
Successfully loaded paths!
Converting training images into numpy arrays...
Loading image paths...
Successfully loaded paths!
Converting ground truth images into numpy arrays...
Total input images: (2594, 384, 512, 3)
Total ground truth images: (2594, 384, 512, 1)
Input image shape: (384, 512, 3)
Ground truth image shape: (384, 512, 1)
2021-10-22 15:02:02.586422: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
2021-10-22 15:02:02.609070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-10-22 15:02:02.617520: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-10-22 15:02:02.633395: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-10-22 15:02:02.638035: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-10-22 15:02:02.645604: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
2021-10-22 15:02:02.650586: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
2021-10-22 15:02:02.657694: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
2021-10-22 15:02:02.665470: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
2021-10-22 15:02:02.670580: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-10-22 15:02:02.674495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-10-22 15:02:02.678407: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-22 15:02:02.688877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-10-22 15:02:02.697252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-10-22 15:02:03.163253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-22 15:02:03.168225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0
2021-10-22 15:02:03.170907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-10-22 15:02:03.173744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6003 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 384, 512, 3) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 384, 512, 16) 448         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 384, 512, 16) 64          conv2d[0][0]
__________________________________________________________________________________________________
leaky_re_lu (LeakyReLU)         (None, 384, 512, 16) 0           batch_normalization[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 384, 512, 16) 2320        leaky_re_lu[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 384, 512, 16) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 384, 512, 16) 64          dropout[0][0]
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 384, 512, 16) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 384, 512, 16) 2320        leaky_re_lu_1[0][0]
__________________________________________________________________________________________________
add (Add)                       (None, 384, 512, 16) 0           conv2d[0][0]
                                                                 conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 192, 256, 32) 4640        add[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 192, 256, 32) 128         conv2d_3[0][0]
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 192, 256, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 192, 256, 32) 9248        leaky_re_lu_2[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 192, 256, 32) 0           conv2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 192, 256, 32) 128         dropout_1[0][0]
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 192, 256, 32) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 192, 256, 32) 9248        leaky_re_lu_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 192, 256, 32) 0           conv2d_3[0][0]
                                                                 conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 96, 128, 64)  18496       add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 96, 128, 64)  256         conv2d_6[0][0]
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 96, 128, 64)  0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 96, 128, 64)  36928       leaky_re_lu_4[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 96, 128, 64)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 96, 128, 64)  256         dropout_2[0][0]
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 96, 128, 64)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 96, 128, 64)  36928       leaky_re_lu_5[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 96, 128, 64)  0           conv2d_6[0][0]
                                                                 conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 48, 64, 128)  73856       add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 48, 64, 128)  512         conv2d_9[0][0]
__________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)       (None, 48, 64, 128)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 48, 64, 128)  147584      leaky_re_lu_6[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 48, 64, 128)  0           conv2d_10[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 48, 64, 128)  512         dropout_3[0][0]
__________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)       (None, 48, 64, 128)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 48, 64, 128)  147584      leaky_re_lu_7[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 48, 64, 128)  0           conv2d_9[0][0]
                                                                 conv2d_11[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 24, 32, 256)  295168      add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 24, 32, 256)  1024        conv2d_12[0][0]
__________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)       (None, 24, 32, 256)  0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 24, 32, 256)  590080      leaky_re_lu_8[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24, 32, 256)  0           conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 24, 32, 256)  1024        dropout_4[0][0]
__________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)       (None, 24, 32, 256)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 24, 32, 256)  590080      leaky_re_lu_9[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 24, 32, 256)  0           conv2d_12[0][0]
                                                                 conv2d_14[0][0]
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 48, 64, 128)  295040      add_4[0][0]
__________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)      (None, 48, 64, 128)  0           conv2d_transpose[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 48, 64, 256)  0           add_3[0][0]
                                                                 leaky_re_lu_10[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 48, 64, 128)  295040      concatenate[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 48, 64, 128)  512         conv2d_15[0][0]
__________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)      (None, 48, 64, 128)  0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 96, 128, 64)  73792       leaky_re_lu_11[0][0]
__________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)      (None, 96, 128, 64)  0           conv2d_transpose_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 96, 128, 128) 0           add_2[0][0]
                                                                 leaky_re_lu_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 96, 128, 64)  73792       concatenate_1[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 96, 128, 64)  256         conv2d_16[0][0]
__________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)      (None, 96, 128, 64)  0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 96, 128, 32)  2080        leaky_re_lu_13[0][0]
__________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)      (None, 96, 128, 32)  0           conv2d_18[0][0]
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 192, 256, 32) 9248        leaky_re_lu_14[0][0]
__________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)      (None, 192, 256, 32) 0           conv2d_transpose_3[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 192, 256, 64) 0           add_1[0][0]
                                                                 leaky_re_lu_15[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 192, 256, 32) 18464       concatenate_2[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 192, 256, 32) 128         conv2d_19[0][0]
__________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)      (None, 192, 256, 32) 0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_transpose_5 (Conv2DTrans (None, 384, 512, 16) 4624        leaky_re_lu_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 96, 128, 1)   577         leaky_re_lu_13[0][0]
__________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)      (None, 384, 512, 16) 0           conv2d_transpose_5[0][0]
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 192, 256, 1)  10          conv2d_17[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 192, 256, 1)  289         leaky_re_lu_16[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 384, 512, 32) 0           add[0][0]
                                                                 leaky_re_lu_17[0][0]
__________________________________________________________________________________________________
add_5 (Add)                     (None, 192, 256, 1)  0           conv2d_transpose_2[0][0]
                                                                 conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 384, 512, 32) 9248        concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 384, 512, 1)  10          add_5[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 384, 512, 1)  289         conv2d_21[0][0]
__________________________________________________________________________________________________
add_6 (Add)                     (None, 384, 512, 1)  0           conv2d_transpose_4[0][0]
                                                                 conv2d_22[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 384, 512, 1)  10          add_6[0][0]
==================================================================================================
Total params: 2,752,305
Trainable params: 2,749,873
Non-trainable params: 2,432
__________________________________________________________________________________________________
Splitting the data...
Got training set!
Generating test and validation...
Train Set Shape         : (1556, 384, 512, 3)
Test Set Shape          : (415, 384, 512, 3)
Validation Set Shape    : (623, 384, 512, 3)
2021-10-22 15:02:09.833777: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/100
2021-10-22 15:02:14.151557: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-10-22 15:02:17.861421: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2021-10-22 15:02:23.373442: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-10-22 15:02:23.935407: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
156/156 [==============================] - 73s 337ms/step - loss: 0.5627 - dice_function: 0.4971 - val_loss: 0.4378 - val_dice_function: 0.5194
Epoch 2/100
156/156 [==============================] - 45s 291ms/step - loss: 0.3387 - dice_function: 0.6697 - val_loss: 0.3494 - val_dice_function: 0.6942
Epoch 3/100
156/156 [==============================] - 46s 297ms/step - loss: 0.2796 - dice_function: 0.7585 - val_loss: 0.4115 - val_dice_function: 0.6648
Epoch 4/100
156/156 [==============================] - 46s 294ms/step - loss: 0.2536 - dice_function: 0.7831 - val_loss: 0.2700 - val_dice_function: 0.7825
Epoch 5/100
156/156 [==============================] - 48s 305ms/step - loss: 0.2334 - dice_function: 0.8098 - val_loss: 0.4238 - val_dice_function: 0.5388
Epoch 6/100
156/156 [==============================] - 47s 304ms/step - loss: 0.2219 - dice_function: 0.8206 - val_loss: 0.2054 - val_dice_function: 0.8380
Epoch 7/100
156/156 [==============================] - 45s 286ms/step - loss: 0.2127 - dice_function: 0.8281 - val_loss: 0.2095 - val_dice_function: 0.8298
Epoch 8/100
156/156 [==============================] - 45s 286ms/step - loss: 0.2085 - dice_function: 0.8313 - val_loss: 0.2285 - val_dice_function: 0.8005
Epoch 9/100
156/156 [==============================] - 46s 295ms/step - loss: 0.2049 - dice_function: 0.8361 - val_loss: 0.2241 - val_dice_function: 0.8368
Epoch 10/100
156/156 [==============================] - 47s 302ms/step - loss: 0.2003 - dice_function: 0.8417 - val_loss: 0.2539 - val_dice_function: 0.7891
Epoch 11/100
156/156 [==============================] - 48s 307ms/step - loss: 0.1949 - dice_function: 0.8471 - val_loss: 0.2079 - val_dice_function: 0.8170
Epoch 12/100
156/156 [==============================] - 52s 334ms/step - loss: 0.1916 - dice_function: 0.8479 - val_loss: 0.1787 - val_dice_function: 0.8595
Epoch 13/100
156/156 [==============================] - 51s 325ms/step - loss: 0.1880 - dice_function: 0.8532 - val_loss: 0.1783 - val_dice_function: 0.8546
Epoch 14/100
156/156 [==============================] - 50s 322ms/step - loss: 0.1835 - dice_function: 0.8570 - val_loss: 0.1937 - val_dice_function: 0.8455
Epoch 15/100
156/156 [==============================] - 53s 338ms/step - loss: 0.1873 - dice_function: 0.8521 - val_loss: 0.2107 - val_dice_function: 0.8338
Epoch 16/100
156/156 [==============================] - 51s 328ms/step - loss: 0.1774 - dice_function: 0.8591 - val_loss: 0.1616 - val_dice_function: 0.8738
Epoch 17/100
156/156 [==============================] - 52s 336ms/step - loss: 0.1745 - dice_function: 0.8635 - val_loss: 0.2002 - val_dice_function: 0.8468
Epoch 18/100
156/156 [==============================] - 52s 337ms/step - loss: 0.1794 - dice_function: 0.8572 - val_loss: 0.2042 - val_dice_function: 0.8469
Epoch 19/100
156/156 [==============================] - 53s 338ms/step - loss: 0.1754 - dice_function: 0.8622 - val_loss: 0.1655 - val_dice_function: 0.8543
Epoch 20/100
156/156 [==============================] - 53s 341ms/step - loss: 0.1746 - dice_function: 0.8623 - val_loss: 0.1611 - val_dice_function: 0.8705
Epoch 21/100
156/156 [==============================] - 47s 299ms/step - loss: 0.1707 - dice_function: 0.8677 - val_loss: 0.1833 - val_dice_function: 0.8576
Epoch 22/100
156/156 [==============================] - 45s 287ms/step - loss: 0.1718 - dice_function: 0.8674 - val_loss: 0.1734 - val_dice_function: 0.8541
Epoch 23/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1678 - dice_function: 0.8693 - val_loss: 0.1661 - val_dice_function: 0.8707
Epoch 24/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1626 - dice_function: 0.8733 - val_loss: 0.1980 - val_dice_function: 0.8519
Epoch 25/100
156/156 [==============================] - 53s 342ms/step - loss: 0.1725 - dice_function: 0.8667 - val_loss: 0.1556 - val_dice_function: 0.8660
Epoch 26/100
156/156 [==============================] - 52s 333ms/step - loss: 0.1677 - dice_function: 0.8701 - val_loss: 0.2074 - val_dice_function: 0.8429
Epoch 27/100
156/156 [==============================] - 51s 327ms/step - loss: 0.1631 - dice_function: 0.8723 - val_loss: 0.2767 - val_dice_function: 0.7510
Epoch 28/100
156/156 [==============================] - 49s 317ms/step - loss: 0.1630 - dice_function: 0.8744 - val_loss: 0.1557 - val_dice_function: 0.8733
Epoch 29/100
156/156 [==============================] - 49s 317ms/step - loss: 0.1594 - dice_function: 0.8758 - val_loss: 0.5361 - val_dice_function: 0.5606
Epoch 30/100
156/156 [==============================] - 49s 313ms/step - loss: 0.1573 - dice_function: 0.8767 - val_loss: 0.1577 - val_dice_function: 0.8765
Epoch 31/100
156/156 [==============================] - 49s 317ms/step - loss: 0.1594 - dice_function: 0.8743 - val_loss: 0.1925 - val_dice_function: 0.8531
Epoch 32/100
156/156 [==============================] - 50s 322ms/step - loss: 0.1576 - dice_function: 0.8759 - val_loss: 0.1591 - val_dice_function: 0.8685
Epoch 33/100
156/156 [==============================] - 51s 327ms/step - loss: 0.1576 - dice_function: 0.8752 - val_loss: 0.1605 - val_dice_function: 0.8742
Epoch 34/100
156/156 [==============================] - 52s 331ms/step - loss: 0.1558 - dice_function: 0.8755 - val_loss: 0.1769 - val_dice_function: 0.8634
Epoch 35/100
156/156 [==============================] - 51s 327ms/step - loss: 0.1544 - dice_function: 0.8773 - val_loss: 0.3440 - val_dice_function: 0.6506
Epoch 36/100
156/156 [==============================] - 52s 334ms/step - loss: 0.1570 - dice_function: 0.8761 - val_loss: 0.1538 - val_dice_function: 0.8784
Epoch 37/100
156/156 [==============================] - 53s 337ms/step - loss: 0.1507 - dice_function: 0.8823 - val_loss: 0.2473 - val_dice_function: 0.7619
Epoch 38/100
156/156 [==============================] - 53s 339ms/step - loss: 0.1541 - dice_function: 0.8788 - val_loss: 0.1497 - val_dice_function: 0.8774
Epoch 39/100
156/156 [==============================] - 53s 342ms/step - loss: 0.1518 - dice_function: 0.8833 - val_loss: 0.1380 - val_dice_function: 0.8879
Epoch 40/100
156/156 [==============================] - 45s 287ms/step - loss: 0.1499 - dice_function: 0.8825 - val_loss: 0.3467 - val_dice_function: 0.6947
Epoch 41/100
156/156 [==============================] - 45s 287ms/step - loss: 0.1500 - dice_function: 0.8842 - val_loss: 0.1393 - val_dice_function: 0.8841
Epoch 42/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1484 - dice_function: 0.8849 - val_loss: 0.1563 - val_dice_function: 0.8722
Epoch 43/100
156/156 [==============================] - 48s 307ms/step - loss: 0.1462 - dice_function: 0.8876 - val_loss: 0.2772 - val_dice_function: 0.7922
Epoch 44/100
156/156 [==============================] - 53s 341ms/step - loss: 0.1467 - dice_function: 0.8862 - val_loss: 0.1418 - val_dice_function: 0.8834
Epoch 45/100
156/156 [==============================] - 52s 332ms/step - loss: 0.1418 - dice_function: 0.8910 - val_loss: 0.2782 - val_dice_function: 0.7359
Epoch 46/100
156/156 [==============================] - 52s 332ms/step - loss: 0.1480 - dice_function: 0.8847 - val_loss: 0.1428 - val_dice_function: 0.8835
Epoch 47/100
156/156 [==============================] - 52s 335ms/step - loss: 0.1427 - dice_function: 0.8877 - val_loss: 0.1421 - val_dice_function: 0.8891
Epoch 48/100
156/156 [==============================] - 45s 288ms/step - loss: 0.1445 - dice_function: 0.8864 - val_loss: 0.1540 - val_dice_function: 0.8806
Epoch 49/100
156/156 [==============================] - 45s 291ms/step - loss: 0.1456 - dice_function: 0.8872 - val_loss: 0.2252 - val_dice_function: 0.8324
Epoch 50/100
156/156 [==============================] - 45s 288ms/step - loss: 0.1402 - dice_function: 0.8900 - val_loss: 0.1421 - val_dice_function: 0.8879
Epoch 51/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1424 - dice_function: 0.8889 - val_loss: 0.2097 - val_dice_function: 0.8452
Epoch 52/100
156/156 [==============================] - 45s 290ms/step - loss: 0.1367 - dice_function: 0.8924 - val_loss: 0.1632 - val_dice_function: 0.8618
Epoch 53/100
156/156 [==============================] - 46s 297ms/step - loss: 0.1396 - dice_function: 0.8934 - val_loss: 0.6023 - val_dice_function: 0.5315
Epoch 54/100
156/156 [==============================] - 50s 317ms/step - loss: 0.1388 - dice_function: 0.8916 - val_loss: 0.1451 - val_dice_function: 0.8785
Epoch 55/100
156/156 [==============================] - 45s 287ms/step - loss: 0.1335 - dice_function: 0.8956 - val_loss: 0.1650 - val_dice_function: 0.8645
Epoch 56/100
156/156 [==============================] - 46s 294ms/step - loss: 0.1363 - dice_function: 0.8933 - val_loss: 0.1563 - val_dice_function: 0.8824
Epoch 57/100
156/156 [==============================] - 48s 306ms/step - loss: 0.1309 - dice_function: 0.8982 - val_loss: 0.1410 - val_dice_function: 0.8862
Epoch 58/100
156/156 [==============================] - 46s 295ms/step - loss: 0.1289 - dice_function: 0.9005 - val_loss: 0.1413 - val_dice_function: 0.8883
Epoch 59/100
156/156 [==============================] - 47s 300ms/step - loss: 0.1301 - dice_function: 0.8985 - val_loss: 0.1495 - val_dice_function: 0.8775
Epoch 60/100
156/156 [==============================] - 45s 290ms/step - loss: 0.1266 - dice_function: 0.9022 - val_loss: 0.8151 - val_dice_function: 0.4339
Epoch 61/100
156/156 [==============================] - 46s 294ms/step - loss: 0.1246 - dice_function: 0.9049 - val_loss: 0.1385 - val_dice_function: 0.8896
Epoch 62/100
156/156 [==============================] - 46s 298ms/step - loss: 0.1243 - dice_function: 0.9048 - val_loss: 0.1566 - val_dice_function: 0.8750
Epoch 63/100
156/156 [==============================] - 46s 296ms/step - loss: 0.1283 - dice_function: 0.9007 - val_loss: 0.1327 - val_dice_function: 0.8924
Epoch 64/100
156/156 [==============================] - 45s 289ms/step - loss: 0.1245 - dice_function: 0.9058 - val_loss: 0.1452 - val_dice_function: 0.8858
Epoch 65/100
156/156 [==============================] - 46s 292ms/step - loss: 0.1245 - dice_function: 0.9032 - val_loss: 0.1851 - val_dice_function: 0.8638
Epoch 66/100
156/156 [==============================] - 45s 291ms/step - loss: 0.1217 - dice_function: 0.9069 - val_loss: 0.4587 - val_dice_function: 0.6147
Epoch 67/100
156/156 [==============================] - 45s 291ms/step - loss: 0.1184 - dice_function: 0.9097 - val_loss: 0.1888 - val_dice_function: 0.8573
Epoch 68/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1224 - dice_function: 0.9056 - val_loss: 0.1623 - val_dice_function: 0.8742
Epoch 69/100
156/156 [==============================] - 45s 290ms/step - loss: 0.1142 - dice_function: 0.9125 - val_loss: 0.1533 - val_dice_function: 0.8843
Epoch 70/100
156/156 [==============================] - 47s 302ms/step - loss: 0.1109 - dice_function: 0.9146 - val_loss: 0.1440 - val_dice_function: 0.8853
Epoch 71/100
156/156 [==============================] - 45s 291ms/step - loss: 0.1122 - dice_function: 0.9128 - val_loss: 0.3605 - val_dice_function: 0.7316
Epoch 72/100
156/156 [==============================] - 45s 287ms/step - loss: 0.1079 - dice_function: 0.9190 - val_loss: 0.1691 - val_dice_function: 0.8671
Epoch 73/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1051 - dice_function: 0.9184 - val_loss: 0.1622 - val_dice_function: 0.8799
Epoch 74/100
156/156 [==============================] - 45s 286ms/step - loss: 0.1071 - dice_function: 0.9186 - val_loss: 0.1477 - val_dice_function: 0.8843
Epoch 75/100
156/156 [==============================] - 45s 288ms/step - loss: 0.1041 - dice_function: 0.9188 - val_loss: 0.1611 - val_dice_function: 0.8771
Epoch 76/100
156/156 [==============================] - 45s 287ms/step - loss: 0.1004 - dice_function: 0.9218 - val_loss: 0.1718 - val_dice_function: 0.8707
Epoch 77/100
156/156 [==============================] - 45s 290ms/step - loss: 0.0966 - dice_function: 0.9264 - val_loss: 0.1590 - val_dice_function: 0.8811
Epoch 78/100
156/156 [==============================] - 45s 291ms/step - loss: 0.0966 - dice_function: 0.9262 - val_loss: 0.1874 - val_dice_function: 0.8646
Epoch 79/100
156/156 [==============================] - 45s 288ms/step - loss: 0.0932 - dice_function: 0.9290 - val_loss: 0.3870 - val_dice_function: 0.7423
Epoch 80/100
156/156 [==============================] - 45s 291ms/step - loss: 0.0937 - dice_function: 0.9285 - val_loss: 0.2172 - val_dice_function: 0.8455
Epoch 81/100
156/156 [==============================] - 46s 294ms/step - loss: 0.0937 - dice_function: 0.9272 - val_loss: 0.1806 - val_dice_function: 0.8735
Epoch 82/100
156/156 [==============================] - 45s 289ms/step - loss: 0.0879 - dice_function: 0.9330 - val_loss: 0.1549 - val_dice_function: 0.8846
Epoch 83/100
156/156 [==============================] - 45s 291ms/step - loss: 0.0867 - dice_function: 0.9335 - val_loss: 0.1634 - val_dice_function: 0.8779
Epoch 84/100
156/156 [==============================] - 46s 294ms/step - loss: 0.0833 - dice_function: 0.9369 - val_loss: 0.2302 - val_dice_function: 0.8446
Epoch 85/100
156/156 [==============================] - 46s 295ms/step - loss: 0.0839 - dice_function: 0.9351 - val_loss: 0.3062 - val_dice_function: 0.8156
Epoch 86/100
156/156 [==============================] - 45s 287ms/step - loss: 0.0786 - dice_function: 0.9407 - val_loss: 0.2158 - val_dice_function: 0.8488
Epoch 87/100
156/156 [==============================] - 48s 308ms/step - loss: 0.0781 - dice_function: 0.9408 - val_loss: 0.1766 - val_dice_function: 0.8790
Epoch 88/100
156/156 [==============================] - 55s 352ms/step - loss: 0.0738 - dice_function: 0.9445 - val_loss: 0.1832 - val_dice_function: 0.8698
Epoch 89/100
156/156 [==============================] - 52s 331ms/step - loss: 0.0793 - dice_function: 0.9390 - val_loss: 0.1625 - val_dice_function: 0.8791
Epoch 90/100
156/156 [==============================] - 52s 331ms/step - loss: 0.0704 - dice_function: 0.9466 - val_loss: 0.1839 - val_dice_function: 0.8750
Epoch 91/100
156/156 [==============================] - 52s 331ms/step - loss: 0.0778 - dice_function: 0.9413 - val_loss: 0.1831 - val_dice_function: 0.8791
Epoch 92/100
156/156 [==============================] - 53s 339ms/step - loss: 0.0738 - dice_function: 0.9438 - val_loss: 0.1917 - val_dice_function: 0.8654
Epoch 93/100
156/156 [==============================] - 46s 296ms/step - loss: 0.0661 - dice_function: 0.9498 - val_loss: 0.1642 - val_dice_function: 0.8836
Epoch 94/100
156/156 [==============================] - 46s 293ms/step - loss: 0.0626 - dice_function: 0.9526 - val_loss: 0.2095 - val_dice_function: 0.8615
Epoch 95/100
156/156 [==============================] - 47s 299ms/step - loss: 0.0638 - dice_function: 0.9511 - val_loss: 0.2781 - val_dice_function: 0.8395
Epoch 96/100
156/156 [==============================] - 47s 299ms/step - loss: 0.0596 - dice_function: 0.9545 - val_loss: 0.2422 - val_dice_function: 0.8567
Epoch 97/100
156/156 [==============================] - 47s 302ms/step - loss: 0.0612 - dice_function: 0.9530 - val_loss: 0.1899 - val_dice_function: 0.8731
Epoch 98/100
156/156 [==============================] - 47s 303ms/step - loss: 0.0791 - dice_function: 0.9401 - val_loss: 0.1756 - val_dice_function: 0.8779
Epoch 99/100
156/156 [==============================] - 47s 302ms/step - loss: 0.0593 - dice_function: 0.9550 - val_loss: 0.1681 - val_dice_function: 0.8828
Epoch 100/100
156/156 [==============================] - 47s 303ms/step - loss: 0.0571 - dice_function: 0.9563 - val_loss: 0.1818 - val_dice_function: 0.8740