{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StyleGANipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8nBPalNRWHL"
      },
      "source": [
        "from math import sqrt\n",
        "import os\n",
        "import random\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable, grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import init\n",
        "from torch.autograd import Function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#from dataset import MultiResolutionDataset\n",
        "#from model import StyledGenerator, Discriminator\n",
        "# train-> model, dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "J6OXicccDijJ",
        "outputId": "21989742-3ea8-4eb5-8711-d22fe5a8984c"
      },
      "source": [
        "#Generate txt\n",
        "!pip install session_info\n",
        "import session_info\n",
        "session_info.show()\n",
        "#!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting session_info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "Collecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8045 sha256=54592495ad89fcf6a99285d1e577e73b64c232688c6f2f343b3255005a7abcfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/ad/14/6a42359351a18337a8683854cfbba99dd782271f2d1767f87f\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib-list, session-info\n",
            "Successfully installed session-info-1.0.0 stdlib-list-0.8.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "PIL                 7.1.2\n",
              "matplotlib          3.2.2\n",
              "numpy               1.19.5\n",
              "session_info        1.0.0\n",
              "torch               1.9.0+cu111\n",
              "torchvision         0.10.0+cu111\n",
              "tqdm                4.62.3\n",
              "umap                0.5.2\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "2f7ece400a652629565c523b34ee61b04afa385c    NA\n",
              "absl                                        NA\n",
              "astor                                       0.8.1\n",
              "astunparse                                  1.6.3\n",
              "bottleneck                                  1.3.2\n",
              "cached_property                             1.5.2\n",
              "certifi                                     2021.05.30\n",
              "cffi                                        1.14.6\n",
              "chardet                                     3.0.4\n",
              "cloudpickle                                 1.3.0\n",
              "cycler                                      0.10.0\n",
              "cython_runtime                              NA\n",
              "dateutil                                    2.8.2\n",
              "debugpy                                     1.0.0\n",
              "decorator                                   4.4.2\n",
              "defusedxml                                  0.7.1\n",
              "dill                                        0.3.4\n",
              "flatbuffers                                 NA\n",
              "gast                                        NA\n",
              "google                                      NA\n",
              "google_auth_httplib2                        NA\n",
              "googleapiclient                             NA\n",
              "h5py                                        3.1.0\n",
              "httplib2                                    0.17.4\n",
              "idna                                        2.10\n",
              "ipykernel                                   4.10.1\n",
              "ipython_genutils                            0.2.0\n",
              "ipywidgets                                  7.6.5\n",
              "joblib                                      1.0.1\n",
              "keras                                       2.6.0\n",
              "keras_preprocessing                         1.1.2\n",
              "kiwisolver                                  1.3.2\n",
              "llvmlite                                    0.34.0\n",
              "mpl_toolkits                                NA\n",
              "numba                                       0.51.2\n",
              "oauth2client                                4.1.3\n",
              "opt_einsum                                  v3.3.0\n",
              "pandas                                      1.1.5\n",
              "pexpect                                     4.8.0\n",
              "pickleshare                                 0.7.5\n",
              "pkg_resources                               NA\n",
              "portpicker                                  NA\n",
              "prompt_toolkit                              1.0.18\n",
              "psutil                                      5.4.8\n",
              "ptyprocess                                  0.7.0\n",
              "pyasn1                                      0.4.8\n",
              "pyasn1_modules                              0.2.8\n",
              "pycparser                                   2.20\n",
              "pydev_ipython                               NA\n",
              "pydevconsole                                NA\n",
              "pydevd                                      2.0.0\n",
              "pydevd_concurrency_analyser                 NA\n",
              "pydevd_file_utils                           NA\n",
              "pydevd_plugins                              NA\n",
              "pydevd_tracing                              NA\n",
              "pydot_ng                                    2.0.0\n",
              "pygments                                    2.6.1\n",
              "pynndescent                                 0.5.5\n",
              "pyparsing                                   2.4.7\n",
              "pytz                                        2018.9\n",
              "requests                                    2.23.0\n",
              "rsa                                         4.7.2\n",
              "scipy                                       1.4.1\n",
              "simplegeneric                               NA\n",
              "sitecustomize                               NA\n",
              "six                                         1.15.0\n",
              "sklearn                                     0.22.2.post1\n",
              "socks                                       1.7.1\n",
              "sphinxcontrib                               NA\n",
              "storemagic                                  NA\n",
              "tblib                                       1.7.0\n",
              "tensorboard                                 2.6.0\n",
              "tensorflow                                  2.6.0\n",
              "tensorflow_probability                      0.14.1\n",
              "termcolor                                   1.1.0\n",
              "tornado                                     5.1.1\n",
              "traitlets                                   5.1.0\n",
              "tree                                        0.1.6\n",
              "typing_extensions                           NA\n",
              "uritemplate                                 3.0.1\n",
              "urllib3                                     1.24.3\n",
              "wcwidth                                     0.2.5\n",
              "wrapt                                       1.12.1\n",
              "yaml                                        3.13\n",
              "zmq                                         22.3.0\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             5.5.0\n",
              "jupyter_client      5.3.5\n",
              "jupyter_core        4.8.1\n",
              "notebook            5.3.1\n",
              "-----\n",
              "Python 3.7.12 (default, Sep 10 2021, 00:21:48) [GCC 7.5.0]\n",
              "Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n",
              "-----\n",
              "Session information updated at 2021-10-30 07:58\n",
              "</pre>\n",
              "</details>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYoJaz11gJBe"
      },
      "source": [
        "#model.py /\n",
        "\n",
        "def init_linear(linear):\n",
        "    \"\"\"\n",
        "    weight initialization, let the linear layers' weights following normal distribution.\n",
        "    :param linear: torch linear layer\n",
        "    \"\"\"\n",
        "    init.xavier_normal(linear.weight)\n",
        "    linear.bias.data.zero_()\n",
        "\n",
        "def init_conv(conv):\n",
        "    \"\"\"\n",
        "    weight initialization, let the conv layers' weights following kaiming_normal distribution.\n",
        "    set the bias equals to zero if it is noe None.\n",
        "    :param linear: torch linear layer\n",
        "    \"\"\"\n",
        "    init.kaiming_normal(conv.weight)\n",
        "    if conv.bias is not None:\n",
        "        conv.bias.data.zero_()\n",
        "\n",
        "class EqualLR:\n",
        "    \"\"\"The class is used to apply equalized learning rate\n",
        "\n",
        "    The calss has no object, but in Python3, it is a default that each class will inherit from object.\n",
        "\n",
        "    Attributes:\n",
        "        name: the name of the parameter used in the pytorch structure. Such as \"weights\" refer to some layes' weight.\n",
        "    \"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        '''compute the model's weights'''\n",
        "        weight = getattr(module, self.name + '_orig') #return the values from the parameters model.name+'_orig'\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel() #fan_in -> kernel_size * kernel_size * in_features\n",
        "        return weight * sqrt(2 / fan_in)\n",
        "\n",
        "    @staticmethod #output function's statistic method. There is no need to instantiate a class\n",
        "    def apply(module, name):\n",
        "        '''apply the class EqualLR'''\n",
        "        fn = EqualLR(name)\n",
        "        weight = getattr(module, name)\n",
        "        del module._parameters[name]\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
        "        module.register_forward_pre_hook(fn)\n",
        "        return fn\n",
        "\n",
        "        #build the EqualLR and get the weight we want.\n",
        "        #delete elements from model._parameters, a dictionary of model.\n",
        "        #add new parater in the model called (name + '_orig') which can be accessed as an attribute.\n",
        "        #add hook:\n",
        "        # same as calculate CAM image if you have tried.\n",
        "        # since some value is gone instantly after the model finishing calculation, so we need hook function\n",
        "        # registers a forward pre-hook on the module.\n",
        "        #the hook will be called every time before forward() is invoked.\n",
        "        #lastly, return the hook we made.\n",
        "\n",
        "    def __call__(self, module, input):\n",
        "        '''make the instances behave like functions and can be called like a function'''\n",
        "        weight = self.compute_weight(module)\n",
        "        setattr(module, self.name, weight)#The setattr() function sets the value of the attribute of an object.\n",
        "\n",
        "def equal_lr(module, name='weight'):\n",
        "    '''apply the EqualLr'''\n",
        "    EqualLR.apply(module, name)\n",
        "    return module\n",
        "\n",
        "class FusedUpsample(nn.Module):\n",
        "    ''' Bilinear upsampling method fused with a de-convolution\n",
        "\n",
        "    After emailing with the author, I got the FusedUpsample is just de-conv + bilinear upsample\n",
        "    From the official implementation of styleGAN tensorflow, you can see the same structure:\n",
        "      https://github.com/NVlabs/stylegan/blob/66813a32aac5045fcde72751522a0c0ba963f6f2/training/networks_stylegan.py#L174\n",
        "\n",
        "    Attributes:\n",
        "      ....\n",
        "    '''\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        weight = torch.randn(in_channel, out_channel, kernel_size, kernel_size)\n",
        "        bias = torch.zeros(out_channel)\n",
        "        fan_in = in_channel * kernel_size * kernel_size\n",
        "        self.multiplier = sqrt(2 / fan_in)\n",
        "        self.weight = nn.Parameter(weight)#A kind of Tensor that is to be considered a module parameter.\n",
        "        self.bias = nn.Parameter(bias)#It's default require grade = True\n",
        "        self.pad = padding\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
        "        weight = (\n",
        "            weight[:, :, 1:, 1:]\n",
        "            + weight[:, :, :-1, 1:]\n",
        "            + weight[:, :, 1:, :-1]\n",
        "            + weight[:, :, :-1, :-1]\n",
        "        ) / 4\n",
        "        out = F.conv_transpose2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
        "        return out\n",
        "\n",
        "        #Bilinear upsampling:\n",
        "        #pad = [1, 1, 1, 1] means the tensor is covered all 0\n",
        "        #example:\n",
        "        # [[2,2]  -> [[0,0,0,0],\n",
        "        # ,[2,3]]    [0,2,2,0],\n",
        "        #         [0,2,2,0],\n",
        "        #         [0,0,0,0]]\n",
        "        #calculate 4 parts weights and average them\n",
        "        #[1:, 1:] -> reject values from the 1st row and col\n",
        "        #[-1, :-1] -> reject values from the last row and col\n",
        "        #[:-1, 1:] -> reject values from the last col and 1st row\n",
        "        #[:-1, :-1] -> reject values from the last col and last row\n",
        "        #applies a 2D transposed convolution operator over an input image composed of several input planes.\n",
        "\n",
        "class FusedDownsample(nn.Module):\n",
        "    ''' General same as above class'''\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        weight = torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
        "        bias = torch.zeros(out_channel)\n",
        "        fan_in = in_channel * kernel_size * kernel_size\n",
        "        self.multiplier = sqrt(2 / fan_in)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "        self.pad = padding\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
        "        weight = (\n",
        "            weight[:, :, 1:, 1:]\n",
        "            + weight[:, :, :-1, 1:]\n",
        "            + weight[:, :, 1:, :-1]\n",
        "            + weight[:, :, :-1, :-1]\n",
        "        ) / 4\n",
        "        out = F.conv2d(input, weight, self.bias, stride=2, padding=self.pad)#applies a 2D convolution over an input image composed of several input planes.\n",
        "        return out\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    '''PixelNorm unify the pixels with the similar colour to be the same'''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, input):\n",
        "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "class BlurFunctionBackward(Function):\n",
        "    ''' BlurFunctionBackward.backward will be called when you use backward on the value computed using gradient\n",
        "\n",
        "    It will be not used during backward for the value computed using the result of the forward\n",
        "    the implementation of backward pass of blur operation\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\n",
        "        \"\"\"\n",
        "        define back backward function calculate gradient\n",
        "\n",
        "        :param ctx: pytorch.ctx is a staticmethod used for saving parameters. It will be taken out when doing backpropagation.\n",
        "        :param grad_output: input by \n",
        "        :param kernel: \n",
        "        :param kernel_flip: \n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "        grad_input = F.conv2d(grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]) #groups is for split the grad_output into several parts\n",
        "        return grad_input\n",
        "\n",
        "        #must be used when saving input or output tensors of the forward to be used later in the backward.\n",
        "        #Anything else, i.e., non-tensors and tensors that are neither input nor output should be stored directly on ctx.\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, gradgrad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors #unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "        grad_input = F.conv2d(gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1])\n",
        "        return grad_input, None, None\n",
        "\n",
        "    #BlurFunctionBackward::backward is for 【double backprop】.\n",
        "    #Improving generalization performance using double backpropagation - Yann LeCun\n",
        "    #Some comments are directly copyed from the address below:\n",
        "    #https://pytorch.org/docs/master/notes/extending.html\n",
        "\n",
        "\n",
        "#For better understanding the BlurFunctionBackward and BlurFunction\n",
        "#check the git issue address:\n",
        "#https://github.com/rosinality/style-based-gan-pytorch/issues/89\n",
        "\n",
        "class BlurFunction(Function):\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, input, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "#Applies the function callable to each element in the tensor,\n",
        "#replacing each element with the value returned by callable.\n",
        "blur = BlurFunction.apply\n",
        "\n",
        "#Making Convolutional Networks Shift-Invariant Again\n",
        "#The blur kernel parameters are fixed\n",
        "class Blur(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
        "        weight = weight.view(1, 1, 3, 3)\n",
        "        weight = weight / weight.sum()\n",
        "        # switch the sequence\n",
        "        # [Comment:]Not quite understand why using flip, found it on the styleGAN paper, needs to revise the paper\n",
        "        weight_flip = torch.flip(weight, [2, 3])\n",
        "        '''\n",
        "        This is typically used to register a buffer\n",
        "          that should not to be considered a model parameter.\n",
        "\n",
        "        For example, BatchNorm's ``running_mean``\n",
        "          is not a parameter, but is part of the persistent state.\n",
        "\n",
        "        Buffers can be accessed as attributes using given names.\n",
        "        '''\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return blur(input, self.weight, self.weight_flip)\n",
        "        # return F.conv2d(input, self.weight, padding=1, groups=input.shape[1])\n",
        "\n",
        "#blog: https://samaelchen.github.io/pytorch-pggan/\n",
        "class EqualConv2d(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Conv2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        linear = nn.Linear(in_dim, out_dim)\n",
        "        linear.weight.data.normal_()\n",
        "        linear.bias.data.zero_()\n",
        "\n",
        "        self.linear = equal_lr(linear)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.linear(input)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        kernel_size,\n",
        "        padding,\n",
        "        kernel_size2=None,\n",
        "        padding2=None,\n",
        "        downsample=False,\n",
        "        fused=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        pad1 = padding\n",
        "        pad2 = padding\n",
        "        if padding2 is not None:\n",
        "            pad2 = padding2\n",
        "\n",
        "        kernel1 = kernel_size\n",
        "        kernel2 = kernel_size\n",
        "        if kernel_size2 is not None:\n",
        "            kernel2 = kernel_size2\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            EqualConv2d(in_channel, out_channel, kernel1, padding=pad1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        if downsample:\n",
        "            if fused:\n",
        "                self.conv2 = nn.Sequential(\n",
        "                    Blur(out_channel),\n",
        "                    FusedDownsample(out_channel, out_channel, kernel2, padding=pad2),\n",
        "                    nn.LeakyReLU(0.2),\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                self.conv2 = nn.Sequential(\n",
        "                    Blur(out_channel),\n",
        "                    EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
        "                    nn.AvgPool2d(2),\n",
        "                    nn.LeakyReLU(0.2),\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            self.conv2 = nn.Sequential(\n",
        "                EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class AdaptiveInstanceNorm(nn.Module):\n",
        "    def __init__(self, in_channel, style_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\n",
        "        self.style = EqualLinear(style_dim, in_channel * 2)\n",
        "\n",
        "        self.style.linear.bias.data[:in_channel] = 1\n",
        "        self.style.linear.bias.data[in_channel:] = 0\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        style = self.style(style).unsqueeze(2).unsqueeze(3)\n",
        "        #Splits a tensor into a specific number of chunks.\n",
        "        gamma, beta = style.chunk(2, 1)\n",
        "\n",
        "        out = self.norm(input)\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
        "\n",
        "    def forward(self, image, noise):\n",
        "        return image + self.weight * noise\n",
        "\n",
        "#Const 4*4*512\n",
        "class ConstantInput(nn.Module):\n",
        "    def __init__(self, channel, size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch = input.shape[0]\n",
        "        out = self.input.repeat(batch, 1, 1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class StyledConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        kernel_size=3,\n",
        "        padding=1,\n",
        "        style_dim=512,\n",
        "        initial=False,\n",
        "        upsample=False,\n",
        "        fused=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if initial:\n",
        "            self.conv1 = ConstantInput(in_channel)\n",
        "\n",
        "        else:\n",
        "            if upsample:\n",
        "                if fused:\n",
        "                    self.conv1 = nn.Sequential(\n",
        "                        FusedUpsample(\n",
        "                            in_channel, out_channel, kernel_size, padding=padding\n",
        "                        ),\n",
        "                        Blur(out_channel),\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    self.conv1 = nn.Sequential(\n",
        "                        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                        EqualConv2d(\n",
        "                            in_channel, out_channel, kernel_size, padding=padding\n",
        "                        ),\n",
        "                        Blur(out_channel),\n",
        "                    )\n",
        "\n",
        "            else:\n",
        "                self.conv1 = EqualConv2d(\n",
        "                    in_channel, out_channel, kernel_size, padding=padding\n",
        "                )\n",
        "\n",
        "        self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        self.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        self.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
        "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, input, style, noise):\n",
        "        out = self.conv1(input)\n",
        "        out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "        out = self.adain1(out, style)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.noise2(out, noise)\n",
        "        out = self.lrelu2(out)\n",
        "        out = self.adain2(out, style)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, code_dim, fused=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.progression = nn.ModuleList(\n",
        "            [\n",
        "                StyledConvBlock(512, 512, 3, 1, initial=True),  # 4\n",
        "                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 8\n",
        "                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 16\n",
        "                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 32\n",
        "                StyledConvBlock(512, 256, 3, 1, upsample=True),  # 64\n",
        "                StyledConvBlock(256, 128, 3, 1, upsample=True, fused=fused),  # 128\n",
        "                StyledConvBlock(128, 64, 3, 1, upsample=True, fused=fused),  # 256\n",
        "                StyledConvBlock(64, 32, 3, 1, upsample=True, fused=fused),  # 512\n",
        "                StyledConvBlock(32, 16, 3, 1, upsample=True, fused=fused),  # 1024\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.to_rgb = nn.ModuleList(\n",
        "            [\n",
        "                EqualConv2d(512, 3, 1),\n",
        "                EqualConv2d(512, 3, 1),\n",
        "                EqualConv2d(512, 3, 1),\n",
        "                EqualConv2d(512, 3, 1),\n",
        "                EqualConv2d(256, 3, 1),\n",
        "                EqualConv2d(128, 3, 1),\n",
        "                EqualConv2d(64, 3, 1),\n",
        "                EqualConv2d(32, 3, 1),\n",
        "                EqualConv2d(16, 3, 1),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # self.blur = Blur()\n",
        "    '''\n",
        "    Author comment:\n",
        "      To do mixing regularization,\n",
        "        you should choice some layers (index),\n",
        "        and use secondary latent codes after that layer.\n",
        "      inject_index and crossover is for implement this.\n",
        "    '''\n",
        "    def forward(self, style, noise, step=0, alpha=-1, mixing_range=(-1, -1)):\n",
        "        #should be torch.randn(batch, 1, 4, 4)\n",
        "        out = noise[0]\n",
        "\n",
        "        #inject_index = [9]\n",
        "        if len(style) < 2:\n",
        "            inject_index = [len(self.progression) + 1]\n",
        "\n",
        "        else:\n",
        "            #list is depend on the step(resolution)\n",
        "            #from list sample len(style)-1 values\n",
        "            inject_index = sorted(random.sample(list(range(step)), len(style) - 1))\n",
        "\n",
        "        crossover = 0\n",
        "\n",
        "        #(conv,to_rgb) = (StyledConvBlock,EqualConv2d)\n",
        "        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
        "            if mixing_range == (-1, -1):\n",
        "                #crossover > number of values in inject_index\n",
        "                #and\n",
        "                #i > inject_index[crossover]\n",
        "                if crossover < len(inject_index) and i > inject_index[crossover]:\n",
        "                    crossover = min(crossover + 1, len(style))\n",
        "                # style_step is the selected style\n",
        "                style_step = style[crossover]\n",
        "\n",
        "            else:\n",
        "                if mixing_range[0] <= i <= mixing_range[1]:\n",
        "                    style_step = style[1]\n",
        "\n",
        "                else:\n",
        "                    style_step = style[0]\n",
        "\n",
        "            if i > 0 and step > 0:\n",
        "                out_prev = out\n",
        "\n",
        "            #conv.forward(input,style,noise)\n",
        "            out = conv(out, style_step, noise[i])\n",
        "\n",
        "\n",
        "            #if i==step cut the iternation\n",
        "            if i == step:\n",
        "                out = to_rgb(out)\n",
        "\n",
        "                if i > 0 and 0 <= alpha < 1:\n",
        "                    #skip the current rgb layer\n",
        "                    skip_rgb = self.to_rgb[i - 1](out_prev)\n",
        "                    #nearest up sampling\n",
        "                    skip_rgb = F.interpolate(skip_rgb, scale_factor=2, mode='nearest')\n",
        "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
        "\n",
        "                break\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class StyledGenerator(nn.Module):\n",
        "    def __init__(self, code_dim=512, n_mlp=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.generator = Generator(code_dim)\n",
        "\n",
        "        layers = [PixelNorm()]\n",
        "        for i in range(n_mlp):\n",
        "            layers.append(EqualLinear(code_dim, code_dim))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        #mapping network f\n",
        "        self.style = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input,\n",
        "        noise=None,\n",
        "        step=0,\n",
        "        alpha=-1,\n",
        "        mean_style=None,\n",
        "        style_weight=0,\n",
        "        mixing_range=(-1, -1),\n",
        "    ):\n",
        "        styles = []\n",
        "        #check input type\n",
        "        #since it needs input to be a list or tuple, because of Mixing operation\n",
        "        #if not mixing, it will get  a tensor\n",
        "        # input should becomes -> [latent_code1, latent_code2] or [latent_code1]\n",
        "        #latent_code1 with a shape of [batch_size,laten_code_size]\n",
        "        if type(input) not in (list, tuple):\n",
        "            input = [input]\n",
        "\n",
        "        for i in input:\n",
        "            #let the latent code go into the mapping network f\n",
        "            styles.append(self.style(i))\n",
        "        #input[0] = latent_code1, shape[0]-> batch_size\n",
        "        batch = input[0].shape[0]\n",
        "\n",
        "        #Add noise, in the original no noise is added\n",
        "        if noise is None:\n",
        "            noise = []\n",
        "            #number of noise = value of step\n",
        "            for i in range(step + 1):\n",
        "                size = 4 * 2 ** i #same as resolution\n",
        "                noise.append(torch.randn(batch, 1, size, size, device=input[0].device))\n",
        "\n",
        "        if mean_style is not None:\n",
        "            styles_norm = []\n",
        "            #normalize the style\n",
        "            for style in styles:\n",
        "                styles_norm.append(mean_style + style_weight * (style - mean_style))\n",
        "\n",
        "            styles = styles_norm\n",
        "\n",
        "        return self.generator(styles, noise, step, alpha, mixing_range=mixing_range)\n",
        "    # used for testin the generator after finish training\n",
        "    def mean_style(self, input):\n",
        "        style = self.style(input).mean(0, keepdim=True)\n",
        "\n",
        "        return style\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, fused=True, from_rgb_activate=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.progression = nn.ModuleList(\n",
        "            [\n",
        "                ConvBlock(16, 32, 3, 1, downsample=True, fused=fused),  # 512\n",
        "                ConvBlock(32, 64, 3, 1, downsample=True, fused=fused),  # 256\n",
        "                ConvBlock(64, 128, 3, 1, downsample=True, fused=fused),  # 128\n",
        "                ConvBlock(128, 256, 3, 1, downsample=True, fused=fused),  # 64\n",
        "                ConvBlock(256, 512, 3, 1, downsample=True),  # 32\n",
        "                ConvBlock(512, 512, 3, 1, downsample=True),  # 16\n",
        "                ConvBlock(512, 512, 3, 1, downsample=True),  # 8\n",
        "                ConvBlock(512, 512, 3, 1, downsample=True),  # 4\n",
        "                ConvBlock(513, 512, 3, 1, 4, 0),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        def make_from_rgb(out_channel):\n",
        "            if from_rgb_activate:\n",
        "                return nn.Sequential(EqualConv2d(3, out_channel, 1), nn.LeakyReLU(0.2))\n",
        "\n",
        "            else:\n",
        "                return EqualConv2d(3, out_channel, 1)\n",
        "\n",
        "        self.from_rgb = nn.ModuleList(\n",
        "            [\n",
        "                make_from_rgb(16),\n",
        "                make_from_rgb(32),\n",
        "                make_from_rgb(64),\n",
        "                make_from_rgb(128),\n",
        "                make_from_rgb(256),\n",
        "                make_from_rgb(512),\n",
        "                make_from_rgb(512),\n",
        "                make_from_rgb(512),\n",
        "                make_from_rgb(512),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # self.blur = Blur()\n",
        "\n",
        "        self.n_layer = len(self.progression)\n",
        "\n",
        "        self.linear = EqualLinear(512, 1)\n",
        "\n",
        "    def forward(self, input, step=0, alpha=-1):\n",
        "        for i in range(step, -1, -1):\n",
        "            index = self.n_layer - i - 1\n",
        "\n",
        "            if i == step:\n",
        "                out = self.from_rgb[index](input)\n",
        "\n",
        "            if i == 0:\n",
        "                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\n",
        "                mean_std = out_std.mean()\n",
        "                mean_std = mean_std.expand(out.size(0), 1, 4, 4)\n",
        "                out = torch.cat([out, mean_std], 1)\n",
        "\n",
        "            out = self.progression[index](out)\n",
        "\n",
        "            if i > 0:\n",
        "                if i == step and 0 <= alpha < 1:\n",
        "                    skip_rgb = F.avg_pool2d(input, 2)\n",
        "                    skip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
        "\n",
        "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
        "\n",
        "        out = out.squeeze(2).squeeze(2)\n",
        "        # print(input.size(), out.size(), step)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYIhfKm-S7Nt"
      },
      "source": [
        "#train\n",
        "def requires_grad(model, flag=True):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "def accumulate(model1, model2, decay=0.999):\n",
        "    par1 = dict(model1.named_parameters())\n",
        "    par2 = dict(model2.named_parameters())\n",
        "\n",
        "    for k in par1.keys():\n",
        "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
        "\n",
        "def sample_data(dataset, batch_size, image_size=4):\n",
        "    dataset.resolution = image_size\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=1, drop_last=True)\n",
        "\n",
        "    return loader\n",
        "\n",
        "def adjust_lr(optimizer, lr):\n",
        "    for group in optimizer.param_groups:\n",
        "        mult = group.get('mult', 1)\n",
        "        group['lr'] = lr * mult\n",
        "\n",
        "def train(args, dataset, generator, discriminator):\n",
        "    step = int(math.log2(args.init_size)) - 2\n",
        "    resolution = 4 * 2 ** step\n",
        "    loader = sample_data(\n",
        "        dataset, args.batch.get(resolution, args.batch_default), resolution\n",
        "    )\n",
        "    data_loader = iter(loader)\n",
        "\n",
        "    adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
        "    adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
        "\n",
        "    pbar = tqdm(range(70_000))\n",
        "\n",
        "    requires_grad(generator, False)\n",
        "    requires_grad(discriminator, True)\n",
        "\n",
        "    disc_loss_val = 0\n",
        "    gen_loss_val = 0\n",
        "    grad_loss_val = 0\n",
        "\n",
        "    alpha = 0\n",
        "    used_sample = 0\n",
        "\n",
        "    max_step = int(math.log2(args.max_size)) - 2\n",
        "    final_progress = False\n",
        "\n",
        "    for i in pbar:\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        alpha = min(1, 1 / args.phase * (used_sample + 1))\n",
        "\n",
        "        if (resolution == args.init_size and args.ckpt is None) or final_progress:\n",
        "            alpha = 1\n",
        "\n",
        "        if used_sample > args.phase * 2:\n",
        "            used_sample = 0\n",
        "            step += 1\n",
        "\n",
        "            if step > max_step:\n",
        "                step = max_step\n",
        "                final_progress = True\n",
        "                ckpt_step = step + 1\n",
        "\n",
        "            else:\n",
        "                alpha = 0\n",
        "                ckpt_step = step\n",
        "\n",
        "            resolution = 4 * 2 ** step\n",
        "\n",
        "            loader = sample_data(\n",
        "                dataset, args.batch.get(resolution, args.batch_default), resolution\n",
        "            )\n",
        "            data_loader = iter(loader)\n",
        "\n",
        "            torch.save(\n",
        "                {\n",
        "                    'generator': generator.module.state_dict(),\n",
        "                    'discriminator': discriminator.module.state_dict(),\n",
        "                    'g_optimizer': g_optimizer.state_dict(),\n",
        "                    'd_optimizer': d_optimizer.state_dict(),\n",
        "                    'g_running': g_running.state_dict(),\n",
        "                },\n",
        "                f'checkpoint/train_step-{ckpt_step}.model',\n",
        "            )\n",
        "            #if ckpt_step == 5:\n",
        "              #!cp \"/content/checkpoint/train_step-5.model\" \"/content/drive/MyDrive/Pre_Trained_Model\"\n",
        "            adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
        "            adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
        "\n",
        "        try:\n",
        "            real_image = next(data_loader)\n",
        "\n",
        "        except (OSError, StopIteration):\n",
        "            data_loader = iter(loader)\n",
        "            real_image = next(data_loader)\n",
        "\n",
        "        used_sample += real_image.shape[0]\n",
        "\n",
        "        b_size = real_image.size(0)\n",
        "        real_image = real_image.cuda()\n",
        "\n",
        "        if args.loss == 'wgan-gp':\n",
        "            real_predict = discriminator(real_image, step=step, alpha=alpha)\n",
        "            real_predict = real_predict.mean() - 0.001 * (real_predict ** 2).mean()\n",
        "            (-real_predict).backward()\n",
        "\n",
        "        elif args.loss == 'r1':\n",
        "            real_image.requires_grad = True\n",
        "            real_scores = discriminator(real_image, step=step, alpha=alpha)\n",
        "            real_predict = F.softplus(-real_scores).mean()\n",
        "            real_predict.backward(retain_graph=True)\n",
        "\n",
        "            grad_real = grad(\n",
        "                outputs=real_scores.sum(), inputs=real_image, create_graph=True\n",
        "            )[0]\n",
        "            grad_penalty = (\n",
        "                grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
        "            ).mean()\n",
        "            grad_penalty = 10 / 2 * grad_penalty\n",
        "            grad_penalty.backward()\n",
        "            if i%10 == 0:\n",
        "                grad_loss_val = grad_penalty.item()\n",
        "\n",
        "        if args.mixing and random.random() < 0.9:\n",
        "            #create a rand tensor with the shape [4,batch_size,laten_code_size]\n",
        "            #Then, split into 4 part each has [1,batch_size,laten_vector_size]\n",
        "            gen_in11, gen_in12, gen_in21, gen_in22 = torch.randn(\n",
        "                4, b_size, code_size, device='cuda'\n",
        "            ).chunk(4, 0)\n",
        "            # make a list, squeeze method delete the first dimension\n",
        "            # gen_in11.shape = [batch_size,laten_code_size]\n",
        "            gen_in1 = [gen_in11.squeeze(0), gen_in12.squeeze(0)]\n",
        "            gen_in2 = [gen_in21.squeeze(0), gen_in22.squeeze(0)]\n",
        "\n",
        "        else:\n",
        "            gen_in1, gen_in2 = torch.randn(2, b_size, code_size, device='cuda').chunk(\n",
        "                2, 0\n",
        "            )\n",
        "            gen_in1 = gen_in1.squeeze(0)\n",
        "            gen_in2 = gen_in2.squeeze(0)\n",
        "\n",
        "        fake_image = generator(gen_in1, step=step, alpha=alpha)\n",
        "        fake_predict = discriminator(fake_image, step=step, alpha=alpha)\n",
        "\n",
        "        if args.loss == 'wgan-gp':\n",
        "            fake_predict = fake_predict.mean()\n",
        "            fake_predict.backward()\n",
        "\n",
        "            eps = torch.rand(b_size, 1, 1, 1).cuda()\n",
        "            x_hat = eps * real_image.data + (1 - eps) * fake_image.data\n",
        "            x_hat.requires_grad = True\n",
        "            hat_predict = discriminator(x_hat, step=step, alpha=alpha)\n",
        "            grad_x_hat = grad(\n",
        "                outputs=hat_predict.sum(), inputs=x_hat, create_graph=True\n",
        "            )[0]\n",
        "            grad_penalty = (\n",
        "                (grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2\n",
        "            ).mean()\n",
        "            grad_penalty = 10 * grad_penalty\n",
        "            grad_penalty.backward()\n",
        "            if i%10 == 0:\n",
        "                grad_loss_val = grad_penalty.item()\n",
        "                disc_loss_val = (-real_predict + fake_predict).item()\n",
        "\n",
        "        elif args.loss == 'r1':\n",
        "            fake_predict = F.softplus(fake_predict).mean()\n",
        "            fake_predict.backward()\n",
        "            if i%10 == 0:\n",
        "                disc_loss_val = (real_predict + fake_predict).item()\n",
        "\n",
        "        d_optimizer.step()\n",
        "\n",
        "        if (i + 1) % n_critic == 0:\n",
        "            generator.zero_grad()\n",
        "\n",
        "            requires_grad(generator, True)\n",
        "            requires_grad(discriminator, False)\n",
        "\n",
        "            fake_image = generator(gen_in2, step=step, alpha=alpha)\n",
        "\n",
        "            predict = discriminator(fake_image, step=step, alpha=alpha)\n",
        "\n",
        "            if args.loss == 'wgan-gp':\n",
        "                loss = -predict.mean()\n",
        "\n",
        "            elif args.loss == 'r1':\n",
        "                loss = F.softplus(-predict).mean()\n",
        "\n",
        "            if i%10 == 0:\n",
        "                gen_loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            g_optimizer.step()\n",
        "            accumulate(g_running, generator.module)\n",
        "\n",
        "            requires_grad(generator, False)\n",
        "            requires_grad(discriminator, True)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            images = []\n",
        "\n",
        "            gen_i, gen_j = args.gen_sample.get(resolution, (10, 5))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(gen_i):\n",
        "                    images.append(\n",
        "                        g_running(\n",
        "                            torch.randn(gen_j, code_size).cuda(), step=step, alpha=alpha\n",
        "                        ).data.cpu()\n",
        "                    )\n",
        "\n",
        "            utils.save_image(\n",
        "                torch.cat(images, 0),\n",
        "                f'sample/{str(i + 1).zfill(6)}.png',\n",
        "                nrow=gen_i,\n",
        "                normalize=True,\n",
        "                range=(-1, 1),\n",
        "            )\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            torch.save(\n",
        "                g_running.state_dict(), f'checkpoint/{str(i + 1).zfill(6)}.model'\n",
        "            )\n",
        "\n",
        "        state_msg = (\n",
        "            f'Size: {4 * 2 ** step}; G: {gen_loss_val:.3f}; D: {disc_loss_val:.3f};'\n",
        "            f' Grad: {grad_loss_val:.3f}; Alpha: {alpha:.5f}'\n",
        "        )\n",
        "\n",
        "        pbar.set_description(state_msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XonsInKVT7K9"
      },
      "source": [
        "# Main operation and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8hmeGZkRSxz"
      },
      "source": [
        "#dataset.py\n",
        "class MultiResolutionDataset(Dataset):\n",
        "    def __init__(self, path, transform, resolution=8):\n",
        "        self.env = lmdb.open(\n",
        "            path,\n",
        "            max_readers=32,\n",
        "            readonly=True,\n",
        "            lock=False,\n",
        "            readahead=False,\n",
        "            meminit=False,\n",
        "        )\n",
        "\n",
        "        if not self.env:\n",
        "            raise IOError('Cannot open lmdb dataset', path)\n",
        "\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            self.length = int(txn.get('length'.encode('utf-8')).decode('utf-8'))\n",
        "\n",
        "        self.resolution = resolution\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        with self.env.begin(write=False) as txn:\n",
        "            key = f'{self.resolution}-{str(index).zfill(5)}'.encode('utf-8')\n",
        "            img_bytes = txn.get(key)\n",
        "\n",
        "        buffer = BytesIO(img_bytes)\n",
        "        img = Image.open(buffer)\n",
        "        img = self.transform(img)\n",
        "\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "h7NGr-PHZFyc",
        "outputId": "5f1ff48e-618e-4cb6-a2aa-e5cf339278a0"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import argparse\n",
        "from io import BytesIO\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "import lmdb\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import functional as trans_fn\n",
        "\n",
        "!unzip /content/drive/MyDrive/Data_Set/keras_png_slices_COMP3710.zip -d /content/data\n",
        "clear_output()\n",
        "\n",
        "def resize_and_convert(img, size, quality=100):\n",
        "    img = trans_fn.resize(img, size, Image.LANCZOS) #Lanczos Re-sample\n",
        "    img = trans_fn.center_crop(img, size) #crop\n",
        "    buffer = BytesIO() # create a data type named BytesIO\n",
        "    img.save(buffer, format='jpeg', quality=quality)\n",
        "    #getvalue used to get the content just be wrote into the disk\n",
        "    val = buffer.getvalue()\n",
        "\n",
        "    return val\n",
        "\n",
        "\n",
        "def resize_multiple(img, sizes=(8, 16, 32, 64, 128, 256, 512, 1024), quality=100):\n",
        "    imgs = []\n",
        "\n",
        "    for size in sizes:\n",
        "        imgs.append(resize_and_convert(img, size, quality))\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def resize_worker(img_file, sizes):\n",
        "    i, file = img_file\n",
        "    img = Image.open(file)\n",
        "    img = img.convert('RGB')\n",
        "    out = resize_multiple(img, sizes=sizes)\n",
        "\n",
        "    return i, out\n",
        "\n",
        "\n",
        "def prepare(transaction, dataset, n_worker, sizes=(8, 16, 32, 64, 128, 256, 512, 1024)):\n",
        "    #function.partial\n",
        "    resize_fn = partial(resize_worker, sizes=sizes)\n",
        "\n",
        "    files = sorted(dataset.imgs, key=lambda x: x[0])\n",
        "    files = [(i, file) for i, (file, label) in enumerate(files)]\n",
        "    total = 0\n",
        "\n",
        "    with multiprocessing.Pool(n_worker) as pool:\n",
        "        for i, imgs in tqdm(pool.imap_unordered(resize_fn, files)):\n",
        "            for size, img in zip(sizes, imgs):\n",
        "                key = f'{size}-{str(i).zfill(5)}'.encode('utf-8')\n",
        "                transaction.put(key, img)\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        transaction.put('length'.encode('utf-8'), str(total).encode('utf-8'))\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(args=[])\n",
        "args.out = '/content/IM'\n",
        "args.n_worker = 2\n",
        "args.path = '/content/data/sample'\n",
        "\n",
        "imgset = datasets.ImageFolder(args.path)\n",
        "\n",
        "with lmdb.open(args.out, map_size=1024 ** 4, readahead=False) as env:\n",
        "    with env.begin(write=True) as txn:\n",
        "        prepare(txn, imgset, args.n_worker, sizes=(8, 16, 32, 64, 128, 256, 512))\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),\n",
        "    ]\n",
        ")\n",
        "dataset = MultiResolutionDataset(args.out, transform = transform, resolution=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:387: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "11328it [02:17, 82.33it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a72b2dfdaa62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     ]\n\u001b[1;32m     83\u001b[0m )\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiResolutionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'MultiResolutionDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvxdh2b5T6v2",
        "outputId": "a2113684-840d-40b1-fa13-7d4a7e972bf8"
      },
      "source": [
        "code_size = 512\n",
        "batch_size = 4 #8\n",
        "n_critic = 1\n",
        "\n",
        "try:\n",
        "  os.mkdir('sample')\n",
        "  os.mkdir('checkpoint')\n",
        "except:\n",
        "  print('file already exist？')\n",
        "parser = argparse.ArgumentParser(description='Progressive Growing of GANs')\n",
        "\n",
        "parser.add_argument(\n",
        "    '--phase',\n",
        "    type=int,\n",
        "    default=600_000,\n",
        "    help='number of samples used for each training phases',\n",
        ")\n",
        "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
        "parser.add_argument('--sched', default=False, action='store_true', help='use lr scheduling')\n",
        "parser.add_argument('--init_size', default=256, type=int, help='initial image size') #256\n",
        "parser.add_argument('--max_size', default=256, type=int, help='max image size')\n",
        "parser.add_argument('--ckpt', default=None, type=str, help='load from previous checkpoints')\n",
        "parser.add_argument(\n",
        "    '--no_from_rgb_activate',\n",
        "    action='store_true',\n",
        "    help='use activate in from_rgb (original implementation)',\n",
        ")\n",
        "parser.add_argument(\n",
        "    '--mixing', action='store_true', help='use mixing regularization'\n",
        ")\n",
        "parser.add_argument(\n",
        "    '--loss',\n",
        "    type=str,\n",
        "    default='wgan-gp',\n",
        "    choices=['wgan-gp', 'r1'],\n",
        "    help='class of gan loss',\n",
        ")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "# training with multiple gpus\n",
        "generator = nn.DataParallel(StyledGenerator(code_size)).cuda()\n",
        "discriminator = nn.DataParallel(\n",
        "    Discriminator(from_rgb_activate=not args.no_from_rgb_activate)\n",
        ").cuda()\n",
        "g_running = StyledGenerator(code_size).cuda()\n",
        "g_running.train(False)\n",
        "\n",
        "g_optimizer = optim.Adam(\n",
        "    generator.module.generator.parameters(), lr=args.lr, betas=(0.0, 0.99)\n",
        ")\n",
        "g_optimizer.add_param_group(\n",
        "    {\n",
        "        'params': generator.module.style.parameters(),\n",
        "        'lr': args.lr * 0.01,\n",
        "        'mult': 0.01,\n",
        "    }\n",
        ")\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "\n",
        "accumulate(g_running, generator.module, 0)\n",
        "args.ckpt = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version3.model'\n",
        "if args.ckpt is not None:\n",
        "    print('load model')\n",
        "    ckpt = torch.load(args.ckpt)\n",
        "\n",
        "    generator.module.load_state_dict(ckpt['generator'])\n",
        "    discriminator.module.load_state_dict(ckpt['discriminator'])\n",
        "    g_running.load_state_dict(ckpt['g_running'])\n",
        "    g_optimizer.load_state_dict(ckpt['g_optimizer'])\n",
        "    d_optimizer.load_state_dict(ckpt['d_optimizer'])\n",
        "\n",
        "if args.sched:\n",
        "    args.lr = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n",
        "    args.batch = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32, 128: 32, 256: 32}\n",
        "\n",
        "else:\n",
        "    args.lr = {}\n",
        "    args.batch = {}\n",
        "\n",
        "args.gen_sample = {512: (8, 4), 1024: (4, 2)}\n",
        "\n",
        "args.batch_default = batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "NPKiBo__sDVf",
        "outputId": "460cfc19-27ef-4265-de99-16302133296b"
      },
      "source": [
        "train(args, dataset, generator, discriminator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Size: 256; G: -11.287; D: 0.638; Grad: 0.748; Alpha: 1.00000:   0%|          | 99/70000 [01:37<18:36:41,  1.04it/s]/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n",
            "Size: 256; G: 9.773; D: -7.431; Grad: 1.227; Alpha: 1.00000:  49%|████▊     | 34016/70000 [9:35:33<10:08:51,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-911ae3823162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-2841ced978bc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, dataset, generator, discriminator)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mrequires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mfake_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_in2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ed276976f684>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, noise, step, alpha, mean_style, style_weight, mixing_range)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyles_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixing_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixing_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m     \u001b[0;31m# used for testin the generator after finish training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmean_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ed276976f684>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, style, noise, step, alpha, mixing_range)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;31m#conv.forward(input,style,noise)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ed276976f684>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, style, noise)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ed276976f684>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, style)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m         return F.instance_norm(\n\u001b[1;32m     58\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0m_verify_spatial_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2326\u001b[0m     return torch.instance_norm(\n\u001b[0;32m-> 2327\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_input_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m     )\n\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsYV0n1XErNT"
      },
      "source": [
        "torch.save(\n",
        "  {\n",
        "      'generator': generator.module.state_dict(),\n",
        "      'discriminator': discriminator.module.state_dict(),\n",
        "      'g_optimizer': g_optimizer.state_dict(),\n",
        "      'd_optimizer': d_optimizer.state_dict(),\n",
        "      'g_running': g_running.state_dict(),\n",
        "  },\n",
        "  f'checkpoint/andy_version91.model',\n",
        ")\n",
        "!cp \"/content/checkpoint/andy_version91.model\" \"/content/drive/MyDrive/Pre_Trained_Model/StyleGAN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_o5ks-zRQ3-"
      },
      "source": [
        "# Generation Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmaAlSWFPCw3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "74e03b89-3937-4294-c874-e7a302013f03"
      },
      "source": [
        "@torch.no_grad()\n",
        "def get_mean_style(generator, device):\n",
        "    # generate 1024 codes ten times and average them\n",
        "    mean_style = None\n",
        "\n",
        "    for i in range(10):\n",
        "        style = generator.mean_style(torch.randn(1024, 512).to(device))\n",
        "\n",
        "        if mean_style is None:\n",
        "            mean_style = style\n",
        "\n",
        "        else:\n",
        "            mean_style += style\n",
        "\n",
        "    mean_style /= 10\n",
        "    return mean_style\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(generator, step, mean_style, n_sample, device):\n",
        "    image = generator(\n",
        "        torch.randn(n_sample, 512).to(device),\n",
        "        step=step,\n",
        "        alpha=1,\n",
        "        mean_style=mean_style,\n",
        "        style_weight=0.7,\n",
        "    )\n",
        "    \n",
        "    return image\n",
        "\n",
        "@torch.no_grad()\n",
        "def style_mixing(generator, step, mean_style, n_source, n_target, device):\n",
        "    source_code = torch.randn(n_source, 512).to(device)\n",
        "    target_code = torch.randn(n_target, 512).to(device)\n",
        "    \n",
        "    shape = 4 * 2 ** step\n",
        "    alpha = 1\n",
        "\n",
        "    images = [torch.zeros(1, 3, shape, shape).to(device) * -1]\n",
        "    images_data = {}\n",
        "    images_data[0] = (0,0)\n",
        "\n",
        "    source_image = generator(\n",
        "        source_code, step=step, alpha=alpha, mean_style=mean_style, style_weight=0.7\n",
        "    )\n",
        "    target_image = generator(\n",
        "        target_code, step=step, alpha=alpha, mean_style=mean_style, style_weight=0.7\n",
        "    )\n",
        "    \n",
        "    images.append(source_image)\n",
        "    for i in range(n_source):\n",
        "      images_data[i+1] = (torch.mean(source_image[i]),torch.std(source_image[i]))\n",
        "    #print('end for source_image')\n",
        "    images_data_length = len(images_data)\n",
        "    #images_data.append()\n",
        "    for i in range(n_target):\n",
        "        image = generator(\n",
        "            [target_code[i].unsqueeze(0).repeat(n_source, 1), source_code],\n",
        "            step=step,\n",
        "            alpha=alpha,\n",
        "            mean_style=mean_style,\n",
        "            style_weight=0.7,\n",
        "            mixing_range=(0, 1),\n",
        "        )\n",
        "\n",
        "        images_data[images_data_length] = (torch.mean(target_image[i].unsqueeze(0)),torch.std(target_image[i].unsqueeze(0)))\n",
        "        for j in range(n_source):\n",
        "          images_data[images_data_length+1+j] = (torch.mean(image[j]),torch.std(image[j]))\n",
        "        images_data_length = len(images_data)\n",
        "\n",
        "        images.append(target_image[i].unsqueeze(0))\n",
        "        images.append(image)\n",
        "\n",
        "    # joint tensor\n",
        "    images = torch.cat(images, 0)\n",
        "    \n",
        "    return images,images_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df455361cc6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mean_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# generate 1024 codes ten times and average them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmean_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeRj-UXZTH6P",
        "outputId": "4293ac8d-9e38-42db-d57d-a82f1d950536"
      },
      "source": [
        "device = 'cuda'\n",
        "test_path = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version4.model'\n",
        "generator = StyledGenerator(512).cuda()\n",
        "generator.load_state_dict(torch.load(test_path)['g_running'])\n",
        "generator.eval()\n",
        "\n",
        "mean_style = get_mean_style(generator, device)\n",
        "step = int(math.log(256, 2)) - 2\n",
        "img = sample(generator, step, mean_style, 3 * 5, device)\n",
        "utils.save_image(img, 'sample.png', nrow=5, normalize=True, range=(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84sS9donQyb_"
      },
      "source": [
        "#Style Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PUOXKZ8Q2bP"
      },
      "source": [
        "device = 'cuda'\n",
        "test_path = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version4.model'\n",
        "generator = StyledGenerator(512).cuda()\n",
        "generator.load_state_dict(torch.load(test_path)['g_running'])\n",
        "generator.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-3l2hQWREcH",
        "outputId": "78160b7d-1256-4d47-e683-7db9a028e33e"
      },
      "source": [
        "step = int(math.log(256, 2)) - 2\n",
        "img,images_data = style_mixing(generator, step, mean_style=None, n_source=3, n_target=3, device=device)\n",
        "utils.save_image(img, 'sample.png', nrow=4, normalize=True, range=(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end for source_image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4PAMGNQqeiq",
        "outputId": "257b3205-38b9-4b10-f610-51b303543f9f"
      },
      "source": [
        "print(images_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: (0, 0), 1: (tensor(-0.7271, device='cuda:0'), tensor(0.3776, device='cuda:0')), 2: (tensor(-0.7557, device='cuda:0'), tensor(0.3072, device='cuda:0')), 3: (tensor(-0.7431, device='cuda:0'), tensor(0.3635, device='cuda:0')), 4: (tensor(-0.7412, device='cuda:0'), tensor(0.3718, device='cuda:0')), 5: (tensor(-0.7412, device='cuda:0'), tensor(0.3654, device='cuda:0')), 6: (tensor(-0.7412, device='cuda:0'), tensor(0.3549, device='cuda:0')), 7: (tensor(-0.7412, device='cuda:0'), tensor(0.3697, device='cuda:0')), 8: (tensor(-0.7404, device='cuda:0'), tensor(0.3547, device='cuda:0')), 9: (tensor(-0.7404, device='cuda:0'), tensor(0.3483, device='cuda:0')), 10: (tensor(-0.7404, device='cuda:0'), tensor(0.3344, device='cuda:0')), 11: (tensor(-0.7404, device='cuda:0'), tensor(0.3548, device='cuda:0')), 12: (tensor(-0.7578, device='cuda:0'), tensor(0.3500, device='cuda:0')), 13: (tensor(-0.7578, device='cuda:0'), tensor(0.3485, device='cuda:0')), 14: (tensor(-0.7578, device='cuda:0'), tensor(0.3357, device='cuda:0')), 15: (tensor(-0.7578, device='cuda:0'), tensor(0.3562, device='cuda:0'))}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm8aPhuvMpK8"
      },
      "source": [
        "# Random Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5RwhmUWMrXM",
        "outputId": "91fa12e6-73e4-4415-b8be-4eaa04dde19e"
      },
      "source": [
        "device = 'cuda'\n",
        "test_path = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version4.model'\n",
        "generator = StyledGenerator(512).cuda()\n",
        "generator.load_state_dict(torch.load(test_path)['g_running'])\n",
        "generator.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqvh_5gxRDOd",
        "outputId": "b4c9409e-83af-4706-a12a-85e7a6483301"
      },
      "source": [
        "#different noise\n",
        "random_seed = 45654897\n",
        "n_sample = 1\n",
        "torch.manual_seed(random_seed)\n",
        "latent_code = torch.randn(n_sample, 512).to(device)\n",
        "step = int(math.log(256, 2)) - 2\n",
        "\n",
        "for j in range(3):\n",
        "  random_seed = j*512\n",
        "  torch.manual_seed(random_seed)\n",
        "\n",
        "  img = generator(\n",
        "    latent_code,\n",
        "    noise= None,\n",
        "    step=step,\n",
        "    alpha=1,\n",
        "    mean_style=None,\n",
        "    style_weight=0,\n",
        "    mixing_range=(-1, -1),\n",
        "  )\n",
        "  utils.save_image(img, 'sample{index}.png'.format(index=j), nrow=1, normalize=True, range=(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo-uzcDps8-U",
        "outputId": "4c05b45a-b35c-4e0a-ade7-e8793bb5b223"
      },
      "source": [
        "# zero noise and full noise\n",
        "random_seed = 19971014\n",
        "n_sample = 1\n",
        "torch.manual_seed(random_seed)\n",
        "latent_code = torch.randn(n_sample, 512).to(device)\n",
        "step = int(math.log(256, 2)) - 2\n",
        "\n",
        "noise = []\n",
        "#number of noise = value of step\n",
        "for i in range(step + 1):\n",
        "  size = 4 * 2 ** i #same as resolution\n",
        "  noise.append(torch.zeros(1, 1, size, size).to(device))\n",
        "  #noise.append(torch.randn(batch, 1, size, size, device=input[0].device))\n",
        "\n",
        "img_noise_zero = generator(\n",
        "  latent_code,\n",
        "  noise= noise,\n",
        "  step=step,\n",
        "  alpha=1,\n",
        "  mean_style=None,\n",
        "  style_weight=0,\n",
        "  mixing_range=(-1, -1),\n",
        ")\n",
        "utils.save_image(img_noise_zero, 'sample_noise_zero.png', nrow=1, normalize=True, range=(-1, 1))\n",
        "\n",
        "img_noise = generator(\n",
        "  latent_code,\n",
        "  noise= None,\n",
        "  step=step,\n",
        "  alpha=1,\n",
        "  mean_style=None,\n",
        "  style_weight=0,\n",
        "  mixing_range=(-1, -1),\n",
        ")\n",
        "utils.save_image(img_noise, 'sample_noise.png', nrow=1, normalize=True, range=(-1, 1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKTMzykAziC0",
        "outputId": "4acbd301-6d5d-4927-f485-15bb69076518"
      },
      "source": [
        "# noise with resolution 8-16 and noise with resolution >16\n",
        "random_seed = 19971014\n",
        "n_sample = 1\n",
        "torch.manual_seed(random_seed)\n",
        "latent_code = torch.randn(n_sample, 512).to(device)\n",
        "step = int(math.log(256, 2)) - 2\n",
        "\n",
        "#8-16\n",
        "noise_coarse_layers = []\n",
        "#number of noise = value of step\n",
        "for i in range(step + 1):\n",
        "  size = 4 * 2 ** i #same as resolution\n",
        "  if i<2:\n",
        "    noise_coarse_layers.append(torch.randn(1, 1, size, size).to(device))\n",
        "  else:\n",
        "    noise_coarse_layers.append(torch.zeros(1, 1, size, size).to(device))\n",
        "\n",
        "#>16\n",
        "noise_fine_layers = []\n",
        "#number of noise = value of step\n",
        "for i in range(step + 1):\n",
        "  size = 4 * 2 ** i #same as resolution\n",
        "  if i>=2:\n",
        "    noise_fine_layers.append(torch.randn(1, 1, size, size).to(device))\n",
        "  else:\n",
        "    noise_fine_layers.append(torch.zeros(1, 1, size, size).to(device))\n",
        "\n",
        "img_coarse_layers = generator(\n",
        "  latent_code,\n",
        "  noise= noise_coarse_layers,\n",
        "  step=step,\n",
        "  alpha=1,\n",
        "  mean_style=None,\n",
        "  style_weight=0,\n",
        "  mixing_range=(-1, -1),\n",
        ")\n",
        "utils.save_image(img_coarse_layers, 'sample_coarse_layers.png', nrow=1, normalize=True, range=(-1, 1))\n",
        "\n",
        "img_fine_layers = generator(\n",
        "  latent_code,\n",
        "  noise= noise_fine_layers,\n",
        "  step=step,\n",
        "  alpha=1,\n",
        "  mean_style=None,\n",
        "  style_weight=0,\n",
        "  mixing_range=(-1, -1),\n",
        ")\n",
        "utils.save_image(img_fine_layers, 'sample_fine_layers.png', nrow=1, normalize=True, range=(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uwXGxn2hTXw"
      },
      "source": [
        "# Gram matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratOui4biYkg"
      },
      "source": [
        "def gram_matrix(input):\n",
        "  '''\n",
        "  input.shape = [C, H, W]\n",
        "  '''\n",
        "  C, H, W = input.size()\n",
        "\n",
        "  features = input.view(C, H * W)  # resise F_XL into \\hat F_XL\n",
        "\n",
        "  G = torch.mm(features, features.t())  # compute the gram product\n",
        "\n",
        "  # we 'normalize' the values of the gram matrix\n",
        "  # by dividing by the number of element in each feature maps.\n",
        "  return G.div(C * H * W)\n",
        "\n",
        "#What I learned from the code\n",
        "\n",
        "def StyleLoss(inputA,inputB):\n",
        "  G_A = gram_matrix(inputA).detach()\n",
        "  G_B = gram_matrix(inputB).detach()\n",
        "  return F.mse_loss(G_A, G_B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci2yO_bHhS_w"
      },
      "source": [
        "device = 'cuda'\n",
        "test_path = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version4.model'\n",
        "generator = StyledGenerator(512).cuda()\n",
        "generator.load_state_dict(torch.load(test_path)['g_running'])\n",
        "generator.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyV1RNIpiDwC",
        "outputId": "466f50e8-7bde-47c5-fa63-2875c2110321"
      },
      "source": [
        "step = int(math.log(256, 2)) - 2\n",
        "img,_ = style_mixing(generator, step, mean_style=None, n_source=1, n_target=2, device=device)\n",
        "utils.save_image(img, 'sample.png', nrow=2, normalize=True, range=(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end for source_image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-kd57sTl_I_",
        "outputId": "aa06b735-a76c-487e-8686-bc01f6b867a4"
      },
      "source": [
        "T = 0\n",
        "for i in range(1000):\n",
        "  step = int(math.log(256, 2)) - 2\n",
        "  img,_ = style_mixing(generator, step, mean_style=None, n_source=1, n_target=2, device=device)\n",
        "  utils.save_image(img, 'sample.png', nrow=2, normalize=True, range=(-1, 1))\n",
        "\n",
        "  MSE_21 = StyleLoss(img[2],img[1]).item()\n",
        "  MSE_31 = StyleLoss(img[3],img[1]).item()\n",
        "  MSE_23 = StyleLoss(img[2],img[3]).item()\n",
        "  #print(MSE_21)\n",
        "  #print(MSE_31)\n",
        "  #print(MSE_23)\n",
        "  J = MSE_31>MSE_23\n",
        "  if J == True:\n",
        "    T = T + 1\n",
        "print(T/1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUli1-efF_m-"
      },
      "source": [
        "# U map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea3bQIkJKknf"
      },
      "source": [
        "!pip install umap-learn\n",
        "import umap.umap_ as umap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYTT2JhDIEeH"
      },
      "source": [
        "device = 'cuda'\n",
        "test_path = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version4.model'\n",
        "generator = StyledGenerator(512).cuda()\n",
        "generator.load_state_dict(torch.load(test_path)['g_running'])\n",
        "generator.eval()\n",
        "mapNet = generator.style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQenMCUTIY-e",
        "outputId": "deb34b04-fc44-4cf2-fada-a6a2c37ab23d"
      },
      "source": [
        "#different noise\n",
        "power = 6\n",
        "random_seed = 512558\n",
        "n_sample = 1\n",
        "outputs  = []\n",
        "step = int(math.log(256, 2)) - 2\n",
        "for i in range(10):\n",
        "  torch.manual_seed(random_seed)\n",
        "  z_i = torch.randn(n_sample, 512)\n",
        "  outputs.append(z_i)\n",
        "latent_code_z = torch.cat(outputs, dim=0).to(device)\n",
        "for i in range(10):\n",
        "  latent_code_z[i][98] = latent_code_z[i][98]+i**power\n",
        "  t = latent_code_z[i].unsqueeze(0)\n",
        "  img = generator(\n",
        "      t,\n",
        "      noise= None,\n",
        "      step=step,\n",
        "      alpha=1,\n",
        "      mean_style=None,\n",
        "      style_weight=0,\n",
        "      mixing_range=(-1, -1),\n",
        "      )\n",
        "  utils.save_image(img, 'sample{index}.png'.format(index=i), nrow=1, normalize=True, range=(-1, 1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
            "  warnings.warn(warning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "uDRCdWYjLsed",
        "outputId": "df8afa9b-52e8-476b-dbea-d3273196e28a"
      },
      "source": [
        "latent_code_w = mapNet(latent_code_z)\n",
        "# take latent_code_w out from gpu to cpu and convert to numpy\n",
        "latent_code_w = latent_code_w.cpu().detach().numpy()\n",
        "reducer = umap.UMAP(random_state = 4565489)  # Create our reducer\n",
        "embedding = reducer.fit_transform(latent_code_w)\n",
        "\n",
        "X = embedding[:, 0]\n",
        "Y = embedding[:, 1]\n",
        "\n",
        "plt.plot(X,Y, 'ro', alpha = 0.5)\n",
        "for i in range(10):\n",
        "    plt.text(X[i], Y[i], str(i))\n",
        "\n",
        "plt.savefig('Umap{power}.jpg'.format(power = power))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/umap/umap_.py:2345: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
            "  \"n_neighbors is larger than the dataset size; truncating to \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xlc6_eYlMbV"
      },
      "source": [
        "# #355 Discession on ED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4caBj4unlS2M"
      },
      "source": [
        "device = 'cuda'\n",
        "test_path = '/content/drive/MyDrive/Pre_Trained_Model/StyleGAN/andy_version4.model'\n",
        "generator = StyledGenerator(512).cuda()\n",
        "generator.load_state_dict(torch.load(test_path)['g_running'])\n",
        "generator.eval()\n",
        "mapNet = generator.style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXMJhJUUlzD4"
      },
      "source": [
        "w_x, w_y, z_x, z_y = [],[],[],[]\n",
        "for i in range(500):\n",
        "  z = torch.randn(2000, 512).to(device)\n",
        "  w = mapNet(z)\n",
        "  z = z.cpu().detach().numpy()\n",
        "  w = w.cpu().detach().numpy()\n",
        "  w_x.append(np.mean(w))\n",
        "  w_y.append(np.std(w))\n",
        "  z_x.append(np.mean(z))\n",
        "  z_y.append(np.std(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "FrmdYESDBIdY",
        "outputId": "1aac020b-6b9a-4ff4-86f8-d8d24bc24838"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot()\n",
        "ax.scatter(w_x, w_y, c='red')\n",
        "#ax.scatter(z_x, z_y, c='green')  # 改变颜色\n",
        "plt.savefig(\"z.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "bq2dUCuvZTO0",
        "outputId": "114f9dca-b029-4667-aa86-9faa68ee8488"
      },
      "source": [
        "a1,a2 = w[:,188],w[:,481]\n",
        "b1,b2 = z[:,188],z[:,481]\n",
        "fig = plt.figure()\n",
        "ax = plt.subplot()\n",
        "#ax.scatter(a1, a2, c='red')\n",
        "ax.scatter(b1, b2, c='green')  # 改变颜色\n",
        "plt.xlabel('column #{188}')\n",
        "plt.ylabel('column #{481}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}