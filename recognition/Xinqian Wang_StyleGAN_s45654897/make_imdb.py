# -*- coding: utf-8 -*-
"""StyleGANipynb（For Test）

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ESymP5t0SlXdN5NAvuzKRmyyBI3GrO4F
"""
from torch.utils.data import Dataset
import argparse
from io import BytesIO
import multiprocessing
from functools import partial
from PIL import Image
import lmdb
from tqdm import tqdm
from torchvision import datasets
from torchvision.transforms import functional as trans_fn

class MultiResolutionDataset(Dataset):
    def __init__(self, path, transform, resolution=8):
        self.env = lmdb.open(
            path,
            max_readers=32,
            readonly=True,
            lock=False,
            readahead=False,
            meminit=False,
        )

        if not self.env:
            raise IOError('Cannot open lmdb dataset', path)

        with self.env.begin(write=False) as txn:
            self.length = int(txn.get('length'.encode('utf-8')).decode('utf-8'))

        self.resolution = resolution
        self.transform = transform

    def __len__(self):
        return self.length

    def __getitem__(self, index):
        with self.env.begin(write=False) as txn:
            key = f'{self.resolution}-{str(index).zfill(5)}'.encode('utf-8')
            img_bytes = txn.get(key)

        buffer = BytesIO(img_bytes)
        img = Image.open(buffer)
        img = self.transform(img)

        return img

def resize_and_convert(img, size, quality=100):
    img = trans_fn.resize(img, size, Image.LANCZOS) #Lanczos Re-sample
    img = trans_fn.center_crop(img, size) #crop
    buffer = BytesIO() # create a data type named BytesIO
    img.save(buffer, format='jpeg', quality=quality)
    #getvalue used to get the content just be wrote into the disk
    val = buffer.getvalue()

    return val


def resize_multiple(img, sizes=(8, 16, 32, 64, 128, 256, 512, 1024), quality=100):
    imgs = []

    for size in sizes:
        imgs.append(resize_and_convert(img, size, quality))

    return imgs


def resize_worker(img_file, sizes):
    i, file = img_file
    img = Image.open(file)
    img = img.convert('RGB')
    out = resize_multiple(img, sizes=sizes)

    return i, out


def prepare(transaction, dataset, n_worker, sizes=(8, 16, 32, 64, 128, 256, 512, 1024)):
    #function.partial
    resize_fn = partial(resize_worker, sizes=sizes)

    files = sorted(dataset.imgs, key=lambda x: x[0])
    files = [(i, file) for i, (file, label) in enumerate(files)]
    total = 0

    with multiprocessing.Pool(n_worker) as pool:
        for i, imgs in tqdm(pool.imap_unordered(resize_fn, files)):
            for size, img in zip(sizes, imgs):
                key = f'{size}-{str(i).zfill(5)}'.encode('utf-8')
                transaction.put(key, img)

            total += 1

        transaction.put('length'.encode('utf-8'), str(total).encode('utf-8'))

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--out', type=str) #args.out = '/content/IM'
  parser.add_argument('--n_worker', type=int, default=8) #args.n_worker = 2
  parser.add_argument('path', type=str) #args.path = '/content/data/sample'
  args = parser.parse_args()
  imgset = datasets.ImageFolder(args.path)
  with lmdb.open(args.out, map_size=1024 ** 4, readahead=False) as env:
    with env.begin(write=True) as txn:
      prepare(txn, imgset, args.n_worker)
