# -*- coding: utf-8 -*-
"""StyleGan_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dihj3PthXafuUdVOItzkZZDal-Qk5dEu
"""

import torch
from torch import nn
from torch.nn import init
from torch.nn import functional as F
from torch.autograd import Function
from math import sqrt
import random

def init_linear(linear):
    """
    weight initialization, let the linear layers' weights following normal distribution.
    :param linear: torch linear layer
    """
    init.xavier_normal(linear.weight)
    linear.bias.data.zero_()

def init_conv(conv):
    """
    weight initialization, let the conv layers' weights following kaiming_normal distribution.
    set the bias equals to zero if it is noe None.
    :param linear: torch linear layer
    """
    init.kaiming_normal(conv.weight)
    if conv.bias is not None:
        conv.bias.data.zero_()

class EqualLR:
    """The class is used to apply equalized learning rate

    The calss has no object, but in Python3, it is a default that each class will inherit from object.

    Attributes:
        name: the name of the parameter used in the pytorch structure. Such as "weights" refer to some layes' weight.
    """
    def __init__(self, name):
        self.name = name

    def compute_weight(self, module):
        '''compute the model's weights'''
        weight = getattr(module, self.name + '_orig') #return the values from the parameters model.name+'_orig'
        fan_in = weight.data.size(1) * weight.data[0][0].numel() #fan_in -> kernel_size * kernel_size * in_features
        return weight * sqrt(2 / fan_in)

    @staticmethod #output function's statistic method. There is no need to instantiate a class
    def apply(module, name):
        '''apply the class EqualLR'''
        fn = EqualLR(name)
        weight = getattr(module, name)
        del module._parameters[name]
        module.register_parameter(name + '_orig', nn.Parameter(weight.data))
        module.register_forward_pre_hook(fn)
        return fn

        #build the EqualLR and get the weight we want.
        #delete elements from model._parameters, a dictionary of model.
        #add new parater in the model called (name + '_orig') which can be accessed as an attribute.
        #add hook:
        # same as calculate CAM image if you have tried.
        # since some value is gone instantly after the model finishing calculation, so we need hook function
        # registers a forward pre-hook on the module.
        #the hook will be called every time before forward() is invoked.
        #lastly, return the hook we made.

    def __call__(self, module, input):
        '''make the instances behave like functions and can be called like a function'''
        weight = self.compute_weight(module)
        setattr(module, self.name, weight)#The setattr() function sets the value of the attribute of an object.

def equal_lr(module, name='weight'):
    '''apply the EqualLr'''
    EqualLR.apply(module, name)
    return module

class FusedUpsample(nn.Module):
    ''' Bilinear upsampling method fused with a de-convolution

    After emailing with the author, I got the FusedUpsample is just de-conv + bilinear upsample
    From the official implementation of styleGAN tensorflow, you can see the same structure:
      https://github.com/NVlabs/stylegan/blob/66813a32aac5045fcde72751522a0c0ba963f6f2/training/networks_stylegan.py#L174

    Attributes:
      ....
    '''
    def __init__(self, in_channel, out_channel, kernel_size, padding=0):
        super().__init__()
        weight = torch.randn(in_channel, out_channel, kernel_size, kernel_size)
        bias = torch.zeros(out_channel)
        fan_in = in_channel * kernel_size * kernel_size
        self.multiplier = sqrt(2 / fan_in)
        self.weight = nn.Parameter(weight)#A kind of Tensor that is to be considered a module parameter.
        self.bias = nn.Parameter(bias)#It's default require grade = True
        self.pad = padding

    def forward(self, input):
        weight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])
        weight = (
            weight[:, :, 1:, 1:]
            + weight[:, :, :-1, 1:]
            + weight[:, :, 1:, :-1]
            + weight[:, :, :-1, :-1]
        ) / 4
        out = F.conv_transpose2d(input, weight, self.bias, stride=2, padding=self.pad)
        return out

        #Bilinear upsampling:
        #pad = [1, 1, 1, 1] means the tensor is covered all 0
        #example:
        # [[2,2]  -> [[0,0,0,0],
        # ,[2,3]]    [0,2,2,0],
        #         [0,2,2,0],
        #         [0,0,0,0]]
        #calculate 4 parts weights and average them
        #[1:, 1:] -> reject values from the 1st row and col
        #[-1, :-1] -> reject values from the last row and col
        #[:-1, 1:] -> reject values from the last col and 1st row
        #[:-1, :-1] -> reject values from the last col and last row
        #applies a 2D transposed convolution operator over an input image composed of several input planes.

class FusedDownsample(nn.Module):
    ''' General same as above class'''
    def __init__(self, in_channel, out_channel, kernel_size, padding=0):
        super().__init__()
        weight = torch.randn(out_channel, in_channel, kernel_size, kernel_size)
        bias = torch.zeros(out_channel)
        fan_in = in_channel * kernel_size * kernel_size
        self.multiplier = sqrt(2 / fan_in)
        self.weight = nn.Parameter(weight)
        self.bias = nn.Parameter(bias)
        self.pad = padding

    def forward(self, input):
        weight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])
        weight = (
            weight[:, :, 1:, 1:]
            + weight[:, :, :-1, 1:]
            + weight[:, :, 1:, :-1]
            + weight[:, :, :-1, :-1]
        ) / 4
        out = F.conv2d(input, weight, self.bias, stride=2, padding=self.pad)#applies a 2D convolution over an input image composed of several input planes.
        return out

class PixelNorm(nn.Module):
    '''PixelNorm unify the pixels with the similar colour to be the same'''
    def __init__(self):
        super().__init__()
    def forward(self, input):
        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)

class BlurFunctionBackward(Function):
    ''' BlurFunctionBackward.backward will be called when you use backward on the value computed using gradient

    It will be not used during backward for the value computed using the result of the forward
    the implementation of backward pass of blur operation
    '''
    @staticmethod
    def forward(ctx, grad_output, kernel, kernel_flip):
      """
      define back backward function calculate gradient

      :param ctx: pytorch.ctx is a staticmethod used for saving parameters. It will be taken out when doing backpropagation.
      :param grad_output: input by 
      :param kernel: 
      :param kernel_flip: 
      """
        ctx.save_for_backward(kernel, kernel_flip)
        grad_input = F.conv2d(grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]) #groups is for split the grad_output into several parts
        return grad_input

        #must be used when saving input or output tensors of the forward to be used later in the backward.
        #Anything else, i.e., non-tensors and tensors that are neither input nor output should be stored directly on ctx.

    # This function has only a single output, so it gets only one gradient
    @staticmethod
    def backward(ctx, gradgrad_output):
        kernel, kernel_flip = ctx.saved_tensors #unpack saved_tensors and initialize all gradients w.r.t. inputs to None.
        grad_input = F.conv2d(gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1])
        return grad_input, None, None

    #BlurFunctionBackward::backward is for 【double backprop】.
    #Improving generalization performance using double backpropagation - Yann LeCun
    #Some comments are directly copyed from the address below:
    #https://pytorch.org/docs/master/notes/extending.html


#For better understanding the BlurFunctionBackward and BlurFunction
#check the git issue address:
#https://github.com/rosinality/style-based-gan-pytorch/issues/89

class BlurFunction(Function):
    
    @staticmethod
    def forward(ctx, input, kernel, kernel_flip):
        ctx.save_for_backward(kernel, kernel_flip)

        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])

        return output

    @staticmethod
    def backward(ctx, grad_output):
        kernel, kernel_flip = ctx.saved_tensors

        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)

        return grad_input, None, None

#Applies the function callable to each element in the tensor,
#replacing each element with the value returned by callable.
blur = BlurFunction.apply

#Making Convolutional Networks Shift-Invariant Again
#The blur kernel parameters are fixed
class Blur(nn.Module):
    def __init__(self, channel):
        super().__init__()

        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)
        weight = weight.view(1, 1, 3, 3)
        weight = weight / weight.sum()
        # switch the sequence
        # [Comment:]Not quite understand why using flip, found it on the styleGAN paper, needs to revise the paper
        weight_flip = torch.flip(weight, [2, 3])
        '''
        This is typically used to register a buffer
          that should not to be considered a model parameter.

        For example, BatchNorm's ``running_mean``
          is not a parameter, but is part of the persistent state.

        Buffers can be accessed as attributes using given names.
        '''
        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))
        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))

    def forward(self, input):
        return blur(input, self.weight, self.weight_flip)
        # return F.conv2d(input, self.weight, padding=1, groups=input.shape[1])

#blog: https://samaelchen.github.io/pytorch-pggan/
class EqualConv2d(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()

        conv = nn.Conv2d(*args, **kwargs)
        conv.weight.data.normal_()
        conv.bias.data.zero_()
        self.conv = equal_lr(conv)

    def forward(self, input):
        return self.conv(input)


class EqualLinear(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()

        linear = nn.Linear(in_dim, out_dim)
        linear.weight.data.normal_()
        linear.bias.data.zero_()

        self.linear = equal_lr(linear)

    def forward(self, input):
        return self.linear(input)


class ConvBlock(nn.Module):
    def __init__(
        self,
        in_channel,
        out_channel,
        kernel_size,
        padding,
        kernel_size2=None,
        padding2=None,
        downsample=False,
        fused=False,
    ):
        super().__init__()

        pad1 = padding
        pad2 = padding
        if padding2 is not None:
            pad2 = padding2

        kernel1 = kernel_size
        kernel2 = kernel_size
        if kernel_size2 is not None:
            kernel2 = kernel_size2

        self.conv1 = nn.Sequential(
            EqualConv2d(in_channel, out_channel, kernel1, padding=pad1),
            nn.LeakyReLU(0.2),
        )

        if downsample:
            if fused:
                self.conv2 = nn.Sequential(
                    Blur(out_channel),
                    FusedDownsample(out_channel, out_channel, kernel2, padding=pad2),
                    nn.LeakyReLU(0.2),
                )

            else:
                self.conv2 = nn.Sequential(
                    Blur(out_channel),
                    EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),
                    nn.AvgPool2d(2),
                    nn.LeakyReLU(0.2),
                )

        else:
            self.conv2 = nn.Sequential(
                EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),
                nn.LeakyReLU(0.2),
            )

    def forward(self, input):
        out = self.conv1(input)
        out = self.conv2(out)

        return out


class AdaptiveInstanceNorm(nn.Module):
    def __init__(self, in_channel, style_dim):
        super().__init__()

        self.norm = nn.InstanceNorm2d(in_channel)
        self.style = EqualLinear(style_dim, in_channel * 2)

        self.style.linear.bias.data[:in_channel] = 1
        self.style.linear.bias.data[in_channel:] = 0

    def forward(self, input, style):
        style = self.style(style).unsqueeze(2).unsqueeze(3)
        #Splits a tensor into a specific number of chunks.
        gamma, beta = style.chunk(2, 1)

        out = self.norm(input)
        out = gamma * out + beta

        return out


class NoiseInjection(nn.Module):
    def __init__(self, channel):
        super().__init__()

        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))

    def forward(self, image, noise):
        return image + self.weight * noise

#Const 4*4*512
class ConstantInput(nn.Module):
    def __init__(self, channel, size=4):
        super().__init__()

        self.input = nn.Parameter(torch.randn(1, channel, size, size))

    def forward(self, input):
        batch = input.shape[0]
        out = self.input.repeat(batch, 1, 1, 1)

        return out


class StyledConvBlock(nn.Module):
    def __init__(
        self,
        in_channel,
        out_channel,
        kernel_size=3,
        padding=1,
        style_dim=512,
        initial=False,
        upsample=False,
        fused=False,
    ):
        super().__init__()

        if initial:
            self.conv1 = ConstantInput(in_channel)

        else:
            if upsample:
                if fused:
                    self.conv1 = nn.Sequential(
                        FusedUpsample(
                            in_channel, out_channel, kernel_size, padding=padding
                        ),
                        Blur(out_channel),
                    )

                else:
                    self.conv1 = nn.Sequential(
                        nn.Upsample(scale_factor=2, mode='nearest'),
                        EqualConv2d(
                            in_channel, out_channel, kernel_size, padding=padding
                        ),
                        Blur(out_channel),
                    )

            else:
                self.conv1 = EqualConv2d(
                    in_channel, out_channel, kernel_size, padding=padding
                )

        self.noise1 = equal_lr(NoiseInjection(out_channel))
        self.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)
        self.lrelu1 = nn.LeakyReLU(0.2)

        self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)
        self.noise2 = equal_lr(NoiseInjection(out_channel))
        self.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)
        self.lrelu2 = nn.LeakyReLU(0.2)

    def forward(self, input, style, noise):
        out = self.conv1(input)
        out = self.noise1(out, noise)
        out = self.lrelu1(out)
        out = self.adain1(out, style)

        out = self.conv2(out)
        out = self.noise2(out, noise)
        out = self.lrelu2(out)
        out = self.adain2(out, style)

        return out


class Generator(nn.Module):
    def __init__(self, code_dim, fused=True):
        super().__init__()

        self.progression = nn.ModuleList(
            [
                StyledConvBlock(512, 512, 3, 1, initial=True),  # 4
                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 8
                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 16
                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 32
                StyledConvBlock(512, 256, 3, 1, upsample=True),  # 64
                StyledConvBlock(256, 128, 3, 1, upsample=True, fused=fused),  # 128
                StyledConvBlock(128, 64, 3, 1, upsample=True, fused=fused),  # 256
                StyledConvBlock(64, 32, 3, 1, upsample=True, fused=fused),  # 512
                StyledConvBlock(32, 16, 3, 1, upsample=True, fused=fused),  # 1024
            ]
        )

        self.to_rgb = nn.ModuleList(
            [
                EqualConv2d(512, 3, 1),
                EqualConv2d(512, 3, 1),
                EqualConv2d(512, 3, 1),
                EqualConv2d(512, 3, 1),
                EqualConv2d(256, 3, 1),
                EqualConv2d(128, 3, 1),
                EqualConv2d(64, 3, 1),
                EqualConv2d(32, 3, 1),
                EqualConv2d(16, 3, 1),
            ]
        )

        # self.blur = Blur()
    '''
    Author comment:
      To do mixing regularization,
        you should choice some layers (index),
        and use secondary latent codes after that layer.
      inject_index and crossover is for implement this.
    '''
    def forward(self, style, noise, step=0, alpha=-1, mixing_range=(-1, -1)):
        #should be torch.randn(batch, 1, 4, 4)
        out = noise[0]

        #inject_index = [9]
        if len(style) < 2:
            inject_index = [len(self.progression) + 1]

        else:
            #list is depend on the step(resolution)
            #from list sample len(style)-1 values
            inject_index = sorted(random.sample(list(range(step)), len(style) - 1))

        crossover = 0

        #(conv,to_rgb) = (StyledConvBlock,EqualConv2d)
        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):
            if mixing_range == (-1, -1):
                #crossover > number of values in inject_index
                #and
                #i > inject_index[crossover]
                if crossover < len(inject_index) and i > inject_index[crossover]:
                    crossover = min(crossover + 1, len(style))
                # style_step is the selected style
                style_step = style[crossover]

            else:
                if mixing_range[0] <= i <= mixing_range[1]:
                    style_step = style[1]

                else:
                    style_step = style[0]

            if i > 0 and step > 0:
                out_prev = out

            #conv.forward(input,style,noise)
            out = conv(out, style_step, noise[i])


            #if i==step cut the iternation
            if i == step:
                out = to_rgb(out)

                if i > 0 and 0 <= alpha < 1:
                    #skip the current rgb layer
                    skip_rgb = self.to_rgb[i - 1](out_prev)
                    #nearest up sampling
                    skip_rgb = F.interpolate(skip_rgb, scale_factor=2, mode='nearest')
                    out = (1 - alpha) * skip_rgb + alpha * out

                break

        return out


class StyledGenerator(nn.Module):
    def __init__(self, code_dim=512, n_mlp=8):
        super().__init__()

        self.generator = Generator(code_dim)

        layers = [PixelNorm()]
        for i in range(n_mlp):
            layers.append(EqualLinear(code_dim, code_dim))
            layers.append(nn.LeakyReLU(0.2))
        #mapping network f
        self.style = nn.Sequential(*layers)

    def forward(
        self,
        input,
        noise=None,
        step=0,
        alpha=-1,
        mean_style=None,
        style_weight=0,
        mixing_range=(-1, -1),
    ):
        styles = []
        #check input type
        #since it needs input to be a list or tuple, because of Mixing operation
        #if not mixing, it will get  a tensor
        # input should becomes -> [latent_code1, latent_code2] or [latent_code1]
        #latent_code1 with a shape of [batch_size,laten_code_size]
        if type(input) not in (list, tuple):
            input = [input]

        for i in input:
            #let the latent code go into the mapping network f
            styles.append(self.style(i))
        #input[0] = latent_code1, shape[0]-> batch_size
        batch = input[0].shape[0]

        #Add noise, in the original no noise is added
        if noise is None:
            noise = []
            #number of noise = value of step
            for i in range(step + 1):
                size = 4 * 2 ** i #same as resolution
                noise.append(torch.randn(batch, 1, size, size, device=input[0].device))

        if mean_style is not None:
            styles_norm = []
            #normalize the style
            for style in styles:
                styles_norm.append(mean_style + style_weight * (style - mean_style))

            styles = styles_norm

        return self.generator(styles, noise, step, alpha, mixing_range=mixing_range)
    # used for testin the generator after finish training
    def mean_style(self, input):
        style = self.style(input).mean(0, keepdim=True)

        return style


class Discriminator(nn.Module):
    def __init__(self, fused=True, from_rgb_activate=False):
        super().__init__()

        self.progression = nn.ModuleList(
            [
                ConvBlock(16, 32, 3, 1, downsample=True, fused=fused),  # 512
                ConvBlock(32, 64, 3, 1, downsample=True, fused=fused),  # 256
                ConvBlock(64, 128, 3, 1, downsample=True, fused=fused),  # 128
                ConvBlock(128, 256, 3, 1, downsample=True, fused=fused),  # 64
                ConvBlock(256, 512, 3, 1, downsample=True),  # 32
                ConvBlock(512, 512, 3, 1, downsample=True),  # 16
                ConvBlock(512, 512, 3, 1, downsample=True),  # 8
                ConvBlock(512, 512, 3, 1, downsample=True),  # 4
                ConvBlock(513, 512, 3, 1, 4, 0),
            ]
        )

        def make_from_rgb(out_channel):
            if from_rgb_activate:
                return nn.Sequential(EqualConv2d(3, out_channel, 1), nn.LeakyReLU(0.2))

            else:
                return EqualConv2d(3, out_channel, 1)

        self.from_rgb = nn.ModuleList(
            [
                make_from_rgb(16),
                make_from_rgb(32),
                make_from_rgb(64),
                make_from_rgb(128),
                make_from_rgb(256),
                make_from_rgb(512),
                make_from_rgb(512),
                make_from_rgb(512),
                make_from_rgb(512),
            ]
        )

        # self.blur = Blur()

        self.n_layer = len(self.progression)

        self.linear = EqualLinear(512, 1)

    def forward(self, input, step=0, alpha=-1):
        for i in range(step, -1, -1):
            index = self.n_layer - i - 1

            if i == step:
                out = self.from_rgb[index](input)

            if i == 0:
                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)
                mean_std = out_std.mean()
                mean_std = mean_std.expand(out.size(0), 1, 4, 4)
                out = torch.cat([out, mean_std], 1)

            out = self.progression[index](out)

            if i > 0:
                if i == step and 0 <= alpha < 1:
                    skip_rgb = F.avg_pool2d(input, 2)
                    skip_rgb = self.from_rgb[index + 1](skip_rgb)

                    out = (1 - alpha) * skip_rgb + alpha * out

        out = out.squeeze(2).squeeze(2)
        # print(input.size(), out.size(), step)
        out = self.linear(out)

        return out