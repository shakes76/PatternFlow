"""
Author: Yaxian Shi
Student No.: 46238119
Training loop of StyleGAN2
"""

import os
import random
from torch.autograd import Variable
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import  DataLoader
import torchvision
from torchvision import datasets, transforms
import pickle as pkl
from tqdm import tqdm 
from torch.utils.data import Dataset
import model
import config

class Train_func():
  """
  Train function for StyleGAN2:
  -- define trian D and train G;
  -- define train loop;
  -- save sample generated by G, change root for result in config.
  """
  def __init__(self, 
               b_size = BATCH_SIZES,                # batch size for training
               z_dim = Z_DIM,                       # dimension of latent vector
               img_dim = IMG_DIMENSION,             # dimension of input and output img 
               data_root = DATA_ROOT,               # data root for dataloader
               device = DEVICE,                     # device for training
               num_workers = NUM_WORKERS,           # number of workers
               img_size = TRAIN_IMG_SIZE,           # img size of training data
               img_resolution = IMG_RESOLUTION,     # output img size
               num_epoch = NUM_EPOCH,               # the number of epoches for training
               pin_memory = PIN_MEMORY,             # pin memory
               result_root = RESULT_ROOT,           # root for saving samples
               lr = LEARNING_RATE                   # learning rate
               ):
    self.b_size = b_size
    self.z_dim = z_dim
    self.w_dim = z_dim
    self.img_dim = img_dim
    self.data_root = data_root
    self.num_workers = num_workers
    self.device = device
    self.img_size = img_size
    self.img_resolution = img_resolution
    self.num_epoch = num_epoch
    self.num_workers = num_workers
    self.pin_memory = pin_memory
    self.result_root = result_root
    # seed everything
    seed = 77
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
      torch.cuda.manual_seed(seed)
      torch.cuda.manual_seed_all(seed)
    # define G network and D network
    self.G = Generator(z_dim=self.z_dim, w_dim=self.w_dim, img_resolution=self.img_resolution, img_dim=self.img_dim).to(device)
    self.D = Discriminator(img_resolution=self.img_resolution).to(device)
    # define learning and optimizer
    self.lr = lr
    self.optimizer_G = optim.Adam(self.G.parameters(), lr=self.lr, betas=(0, 0.99))
    self.optimizer_D = optim.Adam(self.D.parameters(), lr=self.lr, betas=(0, 0.99))
    # save plk
    self.z_sample =  torch.randn(9, self.z_dim).to(self.device)

  def mydataloader(self):
    dataset = datasets.ImageFolder(
    root = self.data_root,
    transform = transforms.Compose([
        transforms.Resize(self.img_size),
        transforms.Grayscale(self.img_dim),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5 for _ in range(self.img_dim)], 
                             std=[0.5 for _ in range(self.img_dim)]),
        transforms.RandomHorizontalFlip()
      ])
    )
    self.dataloader = torch.utils.data.DataLoader(dataset,
                                         batch_size=self.b_size,
                                         num_workers=self.num_workers,
                                         pin_memory=self.pin_memory,
                                         shuffle=True)
    
  def train_D(self, real_images):
    real_images = real_images.to(self.device)
    # generate fake images
    z = torch.randn(self.b_size, self.z_dim).to(self.device)
    # clear the gradients have been stored in previous steps
    self.optimizer_D.zero_grad()
    # D forward
    fake_images = self.G(z)
    real_logits = self.D(real_images.detach())
    fake_logits = self.D(fake_images.detach())
    # maximize logits for real images
    loss_Dreal = torch.mean(F.softplus(-real_logits))    
    # minimize logtis for fake images.
    loss_Dfake = torch.mean(F.softplus(fake_logits))       
    # D loss
    D_loss = loss_Dreal + loss_Dfake
    # backwards and update the parameters of D
    D_loss.backward()
    self.optimizer_D.step()

  def train_G(self):
    # generate fake images
    z = torch.randn(self.batch_size, self.z_dim).to(self.device)
    # clear the gradients have been stored in previous steps
    self.optimizer_G.zero_grad()
    #with torch.cuda.amp.autocast():
    fake_images = self.G(z)
    # forward
    g_logits = self.D(fake_images)
    # calculate the loss of G
    G_loss = torch.mean(F.softplus(-g_logits))
    # backwards and update the parameters of G
    G_loss.backward()
    self.optimizer_G.step()

  def train_loop(self):
    # train loop including save imgs
    self.G.train()
    self.D.train()
    self.mydataloader()
    for epoch in range(self.num_epoch):
      samples = []
      loop = tqdm(self.dataloader)
      for i, data in enumerate(loop):
        real_images = data[0].to(self.device)
        self.train_D(real_images)
        self.train_G

      # save imgs/ epoch
      if (epoch+1)%1==0:
        self.G.eval()
        fake_sample = (self.G(self.z_sample).data + 1) / 2.0
        samples.append(fake_sample.cpu())
        with open(self.result_root + 'train_' + str(epoch+1) + '.pkl', 'wb') as f:
          pkl.dump(samples, f)
        self.G.train()

def main():
  train_f = Train_func()
  train_f.train_loop()

if __name__ == "__main__":
  main()
  