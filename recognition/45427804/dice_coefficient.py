# -*- coding: utf-8 -*-
"""Dice_coefficient.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Pj_0InRPiz-PoipOb7xaCAmDcuYZNUO
"""

'''The Dice coefficient is a ensemble similarity measurement function, usually used to 
calculate the similarity of two samples, with a value range of [0,1]:'''
def dice_coeff(y_true, y_pred):
    
    intersection = tf.math.reduce_sum(y_true * y_pred, axis=[1,2])
    union = tf.math.reduce_sum(y_true, axis=[1,2]) + tf.math.reduce_sum(y_pred, axis=[1,2])
    mean_loss = (2. * intersection) / (union + 1e-6)
    return tf.reduce_mean(mean_loss, axis=0)
    
def dice_coeff_loss(y_true, y_pred):
  return 1- tf.reduce_mean(dice_coeff(y_true, y_pred))

'''context module: it is a pre-activation residual block with two 3X3X3 convolutional layers and a dropout layer (drop = 0.3) in between, and it is followed by 3x3x3 convolutions with input stride 2. The purpose is to allow more features while downing to the aggregation pathway and to reduce the
resolution of the feature maps. There are 5 context modules in the model which are cont1, cont2, cont3, cont4 and cont5'''
inputs = Input(shape=(256, 256, 1))

Conv = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(inputs)

Cont1 = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conv)
D1 = Dropout(0.3)(Cont1)
Cont1 = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D1)

Cont1 = Add()([Conv, Cont1])


down1 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont1)

Cont2 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down1)
D2 = Dropout(0.3)(Cont2)
Cont2 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D2)   
Cont2 = Add()([down1, Cont2])

down2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont2)
    
Cont3 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down2)
D3 = Dropout(0.3)(Cont3)
Cont3 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D3)
Cont3 = Add()([down2, Cont3])

down3 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont3)
    
Cont4 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down3)
D4 = Dropout(0.3)(Cont4)
Cont4 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D4)   
Cont4 = Add()([down3, Cont4])

down4 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont4)

Cont5 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down4)
D5 = Dropout(0.3)(Cont5)
Cont5 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D5)
Cont5 = Add()([down4, Cont5])

up1 = UpSampling2D()(Cont5)
up1 = Conv2D(128, (3, 3), padding='same')(up1)

Conca1 = concatenate([up1, Cont4])

Local1 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca1)
Local1 = Conv2D(128, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(Local1)

up2 = UpSampling2D()(Local1)
up2 = Conv2D(64, (3, 3), padding='same')(up2)

Conca2 = concatenate([up2, Cont3])

Local2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca2)
Local2 = Conv2D(64, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(Local2)

up3 = UpSampling2D()(Local2)
up3 = Conv2D(32, (3, 3), padding='same')(up3)

Conca3 = concatenate([up3, Cont2])

Local3 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca3)
Local3 = Conv2D(32, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(up3)


up4 = UpSampling2D()(Local3)
up4 = Conv2D(16, (3, 3), padding='same')(up4)

Conca4 = concatenate([up4, Cont1])

M10 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca4)

#add all segmentation layers
Seg1 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Local2)
Seg1 = UpSampling2D()(Seg1)
Seg2 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(up3)
Seg3 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(M10)

seg12 = Add()([Seg1, Seg2])
seg12 = UpSampling2D()(seg12)
seg123 = Add()([seg12, Seg3])

final = Conv2D(4, (1, 1), activation="softmax")(seg123)
model_dice = Model(inputs=inputs, outputs=final)

opt = keras.optimizers.Adam(learning_rate=1e-4)

model_dice.compile(optimizer = opt, loss = dice_coeff_loss, metrics = [dice_coeff])

checkpoint_path = "training/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

EPOCHS = 80
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

history2 = model_dice.fit(X_train4D_norm, y_train_labels, epochs=EPOCHS, callbacks=[model_checkpoint_callback],
                          batch_size=8, validation_split=0.2, verbose=2)

!ls {checkpoint_dir}

model_dice.evaluate(X_test4D_norm,  y_test_labels, verbose=2)

pre_dice=model_dice.predict(X_test4D_norm)

dice_coeff(y_test_labels,pre_dice)

#plot the traing loss and validation loss
loss_train = history2.history['loss']
loss_val = history2.history['val_loss']
epochs = range(1,81)
plt.plot(epochs, loss_train, 'g', label='loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training loss and validtion loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#plot dice_coefficient 
loss_train = history2.history['dice_coeff']
loss_val = history2.history['val_dice_coeff']
epochs = range(1,81)
plt.plot(epochs, loss_train, 'g', label='dice_coeff')
plt.plot(epochs, loss_val, 'b', label='val_dice_coeff')
plt.title('Training dice_coeff and val_dice_coeff')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()