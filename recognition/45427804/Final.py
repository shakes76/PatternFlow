# -*- coding: utf-8 -*-
"""demo2_part3_u-net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gFLkjknksZRdOvS96RS3ietCJS12RbQy
"""

"""
Author: ShengChih Lin
Student Number: 45427804
Course: Comp3701 
Task: Segment the OASIS brain data set with an Improved UNet with all labels having 
a minimum Dice similarity coefficient of 0.9 on the test set.
"""


import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Add, Activation, concatenate, Conv2D, Dropout, LeakyReLU, Input, UpSampling2D
from tensorflow.keras.models import Model
from keras.layers import LeakyReLU
from tensorflow import keras
from tensorflow.keras.callbacks import TensorBoard
import matplotlib.pyplot as plt
import glob
import os
from PIL import Image
from tensorflow.keras.utils import to_categorical

!pip install pyyaml h5py  # Required to save models in HDF5 format

#connect to the google drive
from google.colab import drive
drive.mount('/gdrive')

#unzip files on google drive
!unzip '/gdrive/My Drive/keras_png_slices_data.zip' -d '/gdrive/My Drive/comp3710project'

import os
path = "/gdrive/My Drive"
os.chdir(path)
os.listdir(path)

#load data
file_train_y = glob.glob('/gdrive/My Drive/comp3710project/keras_png_slices_data/keras_png_slices_seg_train/*.png')
file_train_y = sorted(file_train_y)

y_train = np.array([np.array(Image.open(fname)) for fname in file_train_y])
print(y_train.shape)

file_test_y = glob.glob('/gdrive/My Drive/comp3710project/keras_png_slices_data/keras_png_slices_seg_test/*.png')
file_test_y = sorted(file_test_y)
y_test = np.array([np.array(Image.open(fname)) for fname in file_test_y])
print(y_test.shape)

file_train_x = glob.glob('/gdrive/My Drive/comp3710project/keras_png_slices_data/keras_png_slices_train/*.png')
file_train_x = sorted(file_train_x)
x_train = np.array([np.array(Image.open(fname)) for fname in file_train_x])
print(x_train.shape)

file_test_x = glob.glob('/gdrive/My Drive/comp3710project/keras_png_slices_data/keras_png_slices_test/*.png')
file_test_x = sorted(file_test_x)
x_test = np.array([np.array(Image.open(fname)) for fname in file_test_x])
print(x_test.shape)

X_train4D = x_train.reshape(x_train.shape[0],256,256,1)#.astype('float32')
X_test4D = x_test.reshape(x_test.shape[0],256,256,1)#.astype('float32')
X_train4D.shape

#normalization
X_train4D_norm = X_train4D/255
X_test4D_norm = X_test4D/255

y_train_labels =  y_train/85
y_test_labels = y_test/85

#if use float32, the RAM will run out
y_train_labels = to_categorical(y = y_train_labels,
                            num_classes = 4,
                            dtype='uint8')#integer type

y_test_labels = to_categorical(y=y_test_labels,
                            num_classes = 4,
                            dtype='uint8')#integer type

#change back to float32
y_train_labels = y_train_labels.astype(np.float32)
y_train_labels.dtype

y_test_labels = y_test_labels.astype(np.float32)
y_test_labels.dtype

# Improved Unet
inputs = Input(shape=(256, 256, 1))

Conv = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(inputs)

'''
context module: it is a pre-activation residual block with two 3X3X3 convolutional layers and a dropout layer (drop = 0.3) 
in between, and it is followed by 3x3x3 convolutions with input stride 2. The purpose is to allow more features while downing 
to the aggregation pathway and to reduce the resolution of the feature maps. There are 5 context modules in the model which 
are cont1, cont2, cont3, cont4 and cont5
'''

Cont1 = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conv)
D1 = Dropout(0.3)(Cont1)
Cont1 = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D1)
Cont1 = Add()([Conv, Cont1])# using Add combine previous convolution

# context module is followed by 3x3x3 convolutions with input stride 2.
down1 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont1)

Cont2 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down1)
D2 = Dropout(0.3)(Cont2)
Cont2 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D2)   
Cont2 = Add()([down1, Cont2])

down2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont2)
    
Cont3 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down2)
D3 = Dropout(0.3)(Cont3)
Cont3 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D3)
Cont3 = Add()([down2, Cont3])

down3 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont3)
    
Cont4 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down3)
D4 = Dropout(0.3)(Cont4)
Cont4 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D4)   
Cont4 = Add()([down3, Cont4])

down4 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont4)

Cont5 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down4)
D5 = Dropout(0.3)(Cont5)
Cont5 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D5)
Cont5 = Add()([down4, Cont5])

'''Upsampling modul: it is to upsampling the low-resolution feature maps and connected by a 3x3x3 convolution that halves the number of feature
maps. There are 4 upsampling modul which are up1, up2,up3, up4'''

up1 = UpSampling2D()(Cont5)
up1 = Conv2D(128, (3, 3), padding='same')(up1)

# concatenate() is to connect upsampling layer and context module
Conca1 = concatenate([up1, Cont4])

'''locolization module: it contains a 3x3x3 convolution and a 1x1x1 convolution which reduces half of number of feature maps.'''
Local1 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca1)
Local1 = Conv2D(128, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(Local1)

up2 = UpSampling2D()(Local1)
up2 = Conv2D(64, (3, 3), padding='same')(up2)

Conca2 = concatenate([up2, Cont3])

Local2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca2)
Local2 = Conv2D(64, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(Local2)

up3 = UpSampling2D()(Local2)
up3 = Conv2D(32, (3, 3), padding='same')(up3)

Conca3 = concatenate([up3, Cont2])

Local3 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca3)
Local3 = Conv2D(32, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(up3)


up4 = UpSampling2D()(Local3)
up4 = Conv2D(16, (3, 3), padding='same')(up4)

Conca4 = concatenate([up4, Cont1])

M10 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca4)

#add all segmentation layers
Seg1 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Local2)
Seg1 = UpSampling2D()(Seg1)
Seg2 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(up3)
Seg3 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(M10)

seg12 = Add()([Seg1, Seg2])
seg12 = UpSampling2D()(seg12)
seg123 = Add()([seg12, Seg3])

final = Conv2D(4, (1, 1), activation="softmax")(seg123)
model = Model(inputs=inputs, outputs=final)

#set learning rates
opt = keras.optimizers.Adam(learning_rate=1e-4)

model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])

history = model.fit(X_train4D_norm, y_train_labels, epochs=15, batch_size=8, validation_split=0.2, verbose=2)

prediction=model.predict(X_test4D_norm)

#prediction
plt.imshow(prediction[470][:,:,1], cmap=plt.cm.gray)

plt.imshow(y_test_labels[470][:,:,1], cmap=plt.cm.gray)

'''The Dice coefficient is a ensemble similarity measurement function, usually used to 
calculate the similarity of two samples, with a value range of [0,1]:'''
def dice_coeff(y_true, y_pred):
    
    intersection = tf.math.reduce_sum(y_true * y_pred, axis=[1,2])
    union = tf.math.reduce_sum(y_true, axis=[1,2]) + tf.math.reduce_sum(y_pred, axis=[1,2])
    mean_loss = (2. * intersection) / (union + 1e-6)
    return tf.reduce_mean(mean_loss, axis=0)
    
def dice_coeff_loss(y_true, y_pred):
  return 1- tf.reduce_mean(dice_coeff(y_true, y_pred))

#dice_codfficient testing
dice_coeff(y_test_labels, prediction)

dice_coeff_loss(y_test_labels,prediction)

'''context module: it is a pre-activation residual block with two 3X3X3 convolutional layers and a dropout layer (drop = 0.3) in between, and it is followed by 3x3x3 convolutions with input stride 2. The purpose is to allow more features while downing to the aggregation pathway and to reduce the
resolution of the feature maps. There are 5 context modules in the model which are cont1, cont2, cont3, cont4 and cont5'''
inputs = Input(shape=(256, 256, 1))

Conv = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(inputs)

Cont1 = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conv)
D1 = Dropout(0.3)(Cont1)
Cont1 = Conv2D(16, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D1)

Cont1 = Add()([Conv, Cont1])


down1 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont1)

Cont2 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down1)
D2 = Dropout(0.3)(Cont2)
Cont2 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D2)   
Cont2 = Add()([down1, Cont2])

down2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont2)
    
Cont3 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down2)
D3 = Dropout(0.3)(Cont3)
Cont3 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D3)
Cont3 = Add()([down2, Cont3])

down3 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont3)
    
Cont4 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down3)
D4 = Dropout(0.3)(Cont4)
Cont4 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D4)   
Cont4 = Add()([down3, Cont4])

down4 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same', strides=(2, 2))(Cont4)

Cont5 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(down4)
D5 = Dropout(0.3)(Cont5)
Cont5 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(D5)
Cont5 = Add()([down4, Cont5])

up1 = UpSampling2D()(Cont5)
up1 = Conv2D(128, (3, 3), padding='same')(up1)

Conca1 = concatenate([up1, Cont4])

Local1 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca1)
Local1 = Conv2D(128, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(Local1)

up2 = UpSampling2D()(Local1)
up2 = Conv2D(64, (3, 3), padding='same')(up2)

Conca2 = concatenate([up2, Cont3])

Local2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca2)
Local2 = Conv2D(64, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(Local2)

up3 = UpSampling2D()(Local2)
up3 = Conv2D(32, (3, 3), padding='same')(up3)

Conca3 = concatenate([up3, Cont2])

Local3 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca3)
Local3 = Conv2D(32, (1, 1), activation=LeakyReLU(alpha=0.01), padding='same')(up3)


up4 = UpSampling2D()(Local3)
up4 = Conv2D(16, (3, 3), padding='same')(up4)

Conca4 = concatenate([up4, Cont1])

M10 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Conca4)

#add all segmentation layers
Seg1 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(Local2)
Seg1 = UpSampling2D()(Seg1)
Seg2 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(up3)
Seg3 = Conv2D(4, (3, 3), activation=LeakyReLU(alpha=0.01), padding='same')(M10)

seg12 = Add()([Seg1, Seg2])
seg12 = UpSampling2D()(seg12)
seg123 = Add()([seg12, Seg3])

final = Conv2D(4, (1, 1), activation="softmax")(seg123)
model_dice = Model(inputs=inputs, outputs=final)

opt = keras.optimizers.Adam(learning_rate=1e-4)

model_dice.compile(optimizer = opt, loss = dice_coeff_loss, metrics = [dice_coeff])

checkpoint_path = "training/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

EPOCHS = 80
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

history2 = model_dice.fit(X_train4D_norm, y_train_labels, epochs=EPOCHS, callbacks=[model_checkpoint_callback],
                          batch_size=8, validation_split=0.2, verbose=2)

!ls {checkpoint_dir}

model_dice.evaluate(X_test4D_norm,  y_test_labels, verbose=2)
#model_dice.load_weights(checkpoint_path)

pre_dice=model_dice.predict(X_test4D_norm)

dice_coeff(y_test_labels,pre_dice)

#plot the traing loss and validation loss
loss_train = history2.history['loss']
loss_val = history2.history['val_loss']
epochs = range(1,81)
plt.plot(epochs, loss_train, 'g', label='loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training loss and validtion loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#plot dice_coefficient 
loss_train = history2.history['dice_coeff']
loss_val = history2.history['val_dice_coeff']
epochs = range(1,81)
plt.plot(epochs, loss_train, 'g', label='dice_coeff')
plt.plot(epochs, loss_val, 'b', label='val_dice_coeff')
plt.title('Training dice_coeff and val_dice_coeff')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

