"""
Contains the VQVAE and PixelCNN models
"""

"""
VQVAE Model
Specifications from this paper:  https://arxiv.org/pdf/1711.00937.pdf
grad copy from enc to decoder
encoder: 
        2 conv layrs
            stride 2
            kernel 4x4
        2 residual blocks
            relu
            conv 3x3
            reulu
            conv 1x1
        (all 256 hidden)
decoder:
        2 residual
        2 transposed conv
adam optimiser
lr 2e^-4
batch size 128

Video explaining the paper on VQVAE: (this really helped me undesrtand what
happened between encoder and decoder)
https://www.youtube.com/watch?v=VZFVUrYcig0

Where in the code I have marked that I wasn't sure whether to have activation
or not I have referenced this code to work out whether I should:
https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb
Also used this source (same repo) for understanding the VQ. The paper doesn't 
really talk about VQ in much detail so this and the video were the most helpful. 
https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py
The model here seems to be a bit different, it has more than 2 conv layers ect.
Also it has implemented it by using classes and build and then a functional way,
which I didn't do.
"""

import tensorflow as tf
# switch for readablity
from tensorflow.keras.layers import *

class ResidualBlock(tf.keras.Model):
    """
        One residual bloc consisting of a relu, a 3x3 convolution, 
        another relu and then a 1x1 convolution

        Why did I make this it's own class:
            I think these actually need to be their own classes because we need 
            2 of everything. So if I was to put them all in one class like I had
            planned I'd need: residual1_ con1_, residual1_conv_2, 
                                residual2_conv1, residual2_conv2
            and then again for the decoder.
            If we were to do it structurally we'd need to repeat the same code 
                4 times as well unless it was generated by a function
            I think the classes with the call function are overkill and
            this architecture might be dummer but it's easier to read.
        
        Sources:
        I used this reference to understand residual layers
        https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec
    """
    def __init__(self, filters=16, inputs=None):
        super(ResidualBlock, self).__init__(inputs)
        self.relu = tf.keras.layers.ReLU()

        self.conv1 = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3,3), 
                            strides=(1,1), padding='same',
                            activation='relu', kernel_initializer='he_uniform')
        self.conv2 = tf.keras.layers.Conv2D(filters=filters, kernel_size=(1,1), 
                            strides=(1,1), padding='same',
                            activation=None, kernel_initializer='he_uniform')

    def call(self, X):
        new_x = self.relu(X)
        new_x = self.conv1(new_x)
        new_x = self.conv2(new_x)
        new_x += X
        return new_x

class Encoder(tf.keras.Model):
    """
    The encoder consists of:
        2x conv layers with stride 2 and kernel 4x4
        2x residual blocks
    """
    def __init__(self, inputs=None):
        super(Encoder, self).__init__(inputs, name="encoder")

        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2), padding='same',
                            activation='relu', kernel_initializer='he_uniform')
        self.conv2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(4,4), strides=(2,2), padding='same',
                            activation=None, kernel_initializer='he_uniform')
        self.resid1 = ResidualBlock(filters=16)
        self.resid2 = ResidualBlock(filters=16)

    def call(self, X):
        X = self.conv1(X)
        X = self.conv2(X)
        X = self.resid1(X)
        X = self.resid2(X)
        return X

class Decoder(tf.keras.Model):
    """
    Decoder consists of:
        2x residual layers
        2x conv transform layers
    """
    def __init__(self, inputs=None):
        super(Decoder, self).__init__(inputs, name="decoder")

        self.resid1 = ResidualBlock(16)
        self.resid2 = ResidualBlock(16)

        self.conv_t_1 = tf.keras.layers.Conv2DTranspose(filters=64, padding='same',
                            kernel_size=(4,4), strides=(2,2), activation='relu', name="s")

        self.conv_t_2 = tf.keras.layers.Conv2DTranspose(filters=1, padding='same',
                            kernel_size=(4,4), strides=(2,2), activation=None, name="f")

    def call(self, X):
        X = self.resid1(X)
        X = self.resid2(X)
        X = self.conv_t_1(X)
        X = self.conv_t_2(X)
        return X

class VQ(tf.keras.Model):
    """
    Used to learn the embedding space
    Input and output dims depend on dataset

    source: https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py
    (see docstring at the top of the file for details)
    """
    def __init__(self, indim, outdim, inputs=None):
        super(VQ, self).__init__(inputs)
        # We need the set of weights we can train as the embedding with 
        # uniform initilisation.
        # Initially I tried to find a layer to hold these, but i couldn't make it
        # work with the Embedding layer. So instead just creating a variable
        # and initialising by hand, making sure its trainble.
        self.emb = tf.Variable(
            initial_value=tf.random_uniform_initializer()(shape=(outdim, indim)), 
            trainable=True, name="emb")


        # From the paper, need to make sure encoder commits to an embedding so output
        # does not grow. 
        self.commitment_loss = 0.25 
        self.input_dims = indim

    def call(self, X):
        X_shape = X.shape
        # Change shape to batch*height*width, channels
        X_reshaped = tf.reshape(X, (X_shape[0]*X_shape[1]*X_shape[2], X_shape[3]))

        # Calculate distances
        # This is the distance between one row of the encoded output to what is
        # in the embedded space. We need to multiply by the closest  setin 
        # embedded space, so first we need to calculate what the distance
        # to each of of the e_i in embedded space is. The measure of distance
        # is normal euclidiean distance (L2 norm (normal sum of squares in 2d))
        # Except don't need to take sqrt since the minimum of the square is the
        # same so we just do (a-b)^2 and leave that. 
        # Also we want to avoid doing the  sum as a for loop so we expand it 
        # into a^2+b^2-2ab and do this along one dimention 
        # This is the same as the math from the source listed in the docstring
        # for this class. 
        # So in the end we have sum of squares across rows of X + sum
        # of squares accross columns of weights - 2*rows X * columns weights
        # keepdims=True means we keep these sums in a spare dimention
        distances = tf.math.reduce_sum(X_reshaped**2, axis=1, keepdims=True) +\
                    tf.math.reduce_sum(self.emb**2, axis=0, keepdims=True) -\
                    2*tf.linalg.matmul(X_reshaped, self.emb)

        # Now we need to pick the e_i with the closest distance for each 
        # line in X, look it up in the table and apply it
        indeces = tf.argmin(distances,axis=1)
        # expand to one hot encoding
        encoded = tf.one_hot(indeces, self.input_dims)
        indeces = tf.reshape(indeces, X.shape[:-1])
        # look up the indces in the embeddings
        quantised = tf.nn.embedding_lookup(tf.transpose(self.emb, [1, 0]), indeces)

        # This model has three loss compoenets which are all added to give L (3)
        # from the paper. 
        # The loss for the enc/decoder loss, the loss for the embedding weights
        # and the commitment loss so the output doenst get out of hand
        # So we need to do our own special sum loss function

        # the source does this differntly but as far as i can tell its just mse error
        # so there is no reason to change it
        # keep embed loss constant, only learn enc/dec loss 
        embed_loss = tf.reduce_mean((tf.stop_gradient(quantised) - X) ** 2)
        # keep enc/dec loss constnat only learn embed loss
        enc_dec_loss = tf.reduce_mean((quantised - tf.stop_gradient(X)) ** 2)
        total_loss = self.commitment_loss * embed_loss +enc_dec_loss

        # now we need to feed the weights straight through without this operation
        # being optimised for (so no grad during this)
        q = X + tf.stop_gradient(quantised-X)

        return total_loss, q, encoded, indeces


class VQVAE(tf.keras.Model):
    """
    Complete VQVAE model with an encode and decoder and vector quantisisation 
    between them.
    Information about VQVAE from:
        The paper:  https://arxiv.org/pdf/1711.00937.pdf
    """
    def __init__(self, inputs=None):
        super(VQVAE, self).__init__(inputs)
        
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.vq = VQ(64, 16)

    def call(self, X):
        latent = self.encoder(X)
        loss, q, encoded, indeces = self.vq(latent) 
        recon = self.decoder(q)
        # we return the sum of the three losses for use in optimiser
        return loss, recon

    """
    Getters for accessing components of the model later
    """
    def get_encoder(self):
        return self.encoder
    
    def get_decoder(self):
        return self.decoder
    
    def get_vq(self):
        return self.vq


### Model for generation
"""
Papers describing PixelCNN and generation:

https://arxiv.org/pdf/1606.05328.pdf

https://arxiv.org/pdf/1601.06759.pdf
This was the main source and descripbes the architecture clearly


their latent space is 32x32x1 ( mine is 62x62x1 )
pixels depend on nearby pixels (above and to the let)
conditional distrbutions moedelled by CNN -> masked filters
we can ignore colors

Sources:
also read some of this expalantion: https://bjlkeng.github.io/posts/pixelcnn/
convolutional layer and a mask, guess it can be a normal mask

referenced this, though it a bit different. The explanation is quite good and
it helped me work out some ambiguities in the paper which are listed in comments 
throughout the code
https://bjlkeng.github.io/posts/pixelcnn/
https://github.com/bjlkeng/sandbox/tree/0b82937888f99d825e5e2f5a2d69132ad7d3d53b/notebooks/pixel_cnn

"""

def maskA(shape):
    """
    Takes a shape (shape of the kernel) and returns a type A mask
    Applied only to first convolution later to restrict only to those already seen
    So this will mask pixels in the kernel up and to the left
    """
    # does this count as using numpy? i havent importe it
    # its just that assignment to tf tensors is unneccesarily complicated
    mask = tf.zeros(shape).numpy()
    half_way = shape[0]//2
    mask[:half_way, ...] = 1.0
    # On middle line, make those to the left 1
    # kernel is square so same halfway, up to halfway = 1
    mask[half_way, :half_way, ...] = 1.0
    return mask

def maskB(shape):
    """
    Takes a shape (shape of the kernel) and returns a type B mask
    Applied to layers except irst, allows autoconnections
    up and left allowed
    """
    mask = tf.zeros(shape).numpy()
    half_way = shape[0]//2
    mask[:half_way, ...] = 1.0
    # center also 1
    mask[half_way, :half_way+1, ...] = 1.0
    return mask

class PixelCNNResidual(tf.keras.Model):
    """
        One residual bloc consisting of a relu, a 1x1 conviultion,
        another relu, a 3x3 convolutio, another relu, a 1x1 convoltuion
        
        as descrived in this paper:https://arxiv.org/pdf/1601.06759.pdf
    """
    def __init__(self, inputs=None):
        super(PixelCNNResidual, self).__init__(inputs)
        self.relu = tf.keras.layers.ReLU()

        self.conv1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), 
                            strides=(1,1), padding='same',
                            activation='relu', kernel_initializer='he_uniform')

        # the diagram in the paper is a bit ambigious but i think the 3x3 mask B
        # is inside the residual block
        # referring to this: https://bjlkeng.github.io/posts/pixelcnn/
        # it is masked and inside the risidual
        self.conv2 = MaskedConv2D((3,3),"B", filters=128//2)

        self.conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), 
                            strides=(1,1), padding='same',
                            activation='relu', kernel_initializer='he_uniform')

    def call(self, X):
        new_x = self.relu(X)
        new_x = self.conv1(new_x)
        new_x = self.conv2(new_x)
        new_x = self.conv3(new_x)
        new_x += X
        return new_x


class MaskedConv2D(tf.keras.Model):
    """
    A conv2D layer which applies a mask of type a or b to it's kernel
    first layer has kernel size 7x7 others have 3x3 or 1x1
    """
    def __init__(self, kernel_size, mask_type, filters=128, inputs=None):
        super(MaskedConv2D, self).__init__(inputs)
        self.mask_type  = mask_type #"A" or "B"
        # I assume relu activation but not explicitely stated in paper
        self.conv = tf.keras.layers.Conv2D(filters=128, kernel_size=kernel_size, 
                            strides=(1,1), padding='same',
                            activation='relu', kernel_initializer='he_uniform')

    def build(self, inputs):
        """
        Kernel won't exist until we build the layer so to be able to get the kernel
        get its shape and make the mask and then set the kernel to be used
        we need this build funciton

        https://stackoverflow.com/questions/57993201/how-to-assign-a-result-of-an-operation-to-layer-kernel
        
        """
        self.conv.build(inputs) # makes the kernel
        self.kernel_shape = self.conv.kernel.shape

        # make the mask
        if self.mask_type == "A":
            self.mask = maskA(self.kernel_shape)
        else:
            self.mask = maskB(self.kernel_shape)
    
    def call(self, X):
        # set the kernel to be the kernle && the mask
        self.conv.kernel.assign(self.conv.kernel * self.mask)
        X = self.conv(X)
        return X


    
class PixelCNN(tf.keras.Model):
    """
    7x7 conv
    mask A
    residuals (doesnt say how many)
        conv 3x3 mask B
        resul
    2x conv 1x1 mask B
    output layer, sigmoid for minst
    """
    def __init__(self, inputs=None):
        super(PixelCNN, self).__init__(inputs)
        self.masked_conv_1 = MaskedConv2D((7,7),"A")

        #TODO is there a better way?
        self.list_resid_layers = []
        for i in range(5):
            self.list_resid_layers += [PixelCNNResidual()]

        self.masked_conv_2 = MaskedConv2D((1,1),"B")
        self.masked_conv_3 = MaskedConv2D((1,1),"B")

        # output layer
        self.conv = tf.keras.layers.Conv2D(filters=128, kernel_size=(1,1), 
                            strides=(1,1), padding='same',
                            activation='sigmoid', kernel_initializer='he_uniform')

    def call(self, X):
        # One hot the data same as in VQ layer
        X = tf.one_hot(X, 64)
        X = self.masked_conv_1(X)
        for layer in self.list_resid_layers:
            X = layer(X)
        X = self.masked_conv_2(X)
        X = self.masked_conv_3(X)
        X = self.conv(X)
        return X